{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 523,
      "metadata": {
        "id": "odj1Coq5H080"
      },
      "outputs": [],
      "source": [
        "#@title ##### License { display-mode: \"form\" }\n",
        "# Copyright 2019 DeepMind Technologies Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOOzDGYAZcW3"
      },
      "source": [
        "# OpenSpiel\n",
        "\n",
        "* This Colab gets you started with installing OpenSpiel and its dependencies.\n",
        "* OpenSpiel is a framework for reinforcement learning in games.\n",
        "* The instructions are adapted from [here](https://github.com/deepmind/open_spiel/blob/master/docs/install.md)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC6kQBzWahEF"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2_Vbijh4FlZ"
      },
      "source": [
        "Install OpenSpiel via pip:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 524,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKmGDQKGLE4S",
        "outputId": "eaf7cf42-ee31-4576-a570-d6fe87c52935"
      },
      "outputs": [],
      "source": [
        "!rm -r figure\n",
        "!rm -r 0\n",
        "!mkdir figure\n",
        "!mkdir 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 525,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu102.html\n",
            "Requirement already satisfied: torch-geometric in /home/bhandk/.local/lib/python3.8/site-packages (2.0.3)\n",
            "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from torch-geometric) (5.3.1)\n",
            "Requirement already satisfied: pandas in /home/bhandk/.local/lib/python3.8/site-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from torch-geometric) (2.22.0)\n",
            "Requirement already satisfied: scipy in /home/bhandk/.local/lib/python3.8/site-packages (from torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: scikit-learn in /home/bhandk/.local/lib/python3.8/site-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: yacs in /home/bhandk/.local/lib/python3.8/site-packages (from torch-geometric) (0.1.8)\n",
            "Requirement already satisfied: jinja2 in /home/bhandk/.local/lib/python3.8/site-packages (from torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: rdflib in /home/bhandk/.local/lib/python3.8/site-packages (from torch-geometric) (6.1.1)\n",
            "Requirement already satisfied: numpy in /home/bhandk/.local/lib/python3.8/site-packages (from torch-geometric) (1.22.2)\n",
            "Requirement already satisfied: tqdm in /home/bhandk/.local/lib/python3.8/site-packages (from torch-geometric) (4.63.0)\n",
            "Requirement already satisfied: pyparsing in /home/bhandk/.local/lib/python3.8/site-packages (from torch-geometric) (3.0.7)\n",
            "Requirement already satisfied: networkx in /home/bhandk/.local/lib/python3.8/site-packages (from torch-geometric) (2.6.3)\n",
            "Requirement already satisfied: googledrivedownloader in /home/bhandk/.local/lib/python3.8/site-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/bhandk/.local/lib/python3.8/site-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/bhandk/.local/lib/python3.8/site-packages (from pandas->torch-geometric) (2021.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /home/bhandk/.local/lib/python3.8/site-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/bhandk/.local/lib/python3.8/site-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/bhandk/.local/lib/python3.8/site-packages (from jinja2->torch-geometric) (2.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from rdflib->torch-geometric) (45.2.0)\n",
            "Requirement already satisfied: isodate in /home/bhandk/.local/lib/python3.8/site-packages (from rdflib->torch-geometric) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->torch-geometric) (1.14.0)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu102.html\n",
            "Requirement already satisfied: torch-sparse in /home/bhandk/.local/lib/python3.8/site-packages (0.6.12)\n",
            "Requirement already satisfied: scipy in /home/bhandk/.local/lib/python3.8/site-packages (from torch-sparse) (1.8.0)\n",
            "Requirement already satisfied: numpy<1.25.0,>=1.17.3 in /home/bhandk/.local/lib/python3.8/site-packages (from scipy->torch-sparse) (1.22.2)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu102.html\n",
            "Requirement already satisfied: torch-scatter in /home/bhandk/.local/lib/python3.8/site-packages (2.0.9)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu102.html\n",
            "Requirement already satisfied: torch-cluster in /home/bhandk/.local/lib/python3.8/site-packages (1.5.9)\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu102.html\n",
            "Requirement already satisfied: torch-spline-conv in /home/bhandk/.local/lib/python3.8/site-packages (1.2.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install torch-geometric -f https://pytorch-geometric.com/whl/torch-1.10.0+cu102.html \n",
        "! pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.10.0+cu102.html \n",
        "! pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.10.0+cu102.html \n",
        "! pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.10.0+cu102.html \n",
        "! pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.10.0+cu102.html "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npECI_8fgzuh"
      },
      "source": [
        "# Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 526,
      "metadata": {
        "id": "GCaO4gSLdwat"
      },
      "outputs": [],
      "source": [
        "# Copyright 2019 DeepMind Technologies Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Reinforcement Learning (RL) Environment for Open Spiel.\n",
        "\n",
        "This module wraps Open Spiel Python interface providing an RL-friendly API. It\n",
        "covers both turn-based and simultaneous move games. Interactions between agents\n",
        "and the underlying game occur mostly through the `reset` and `step` methods,\n",
        "which return a `TimeStep` structure (see its docstrings for more info).\n",
        "\n",
        "The following example illustrates the interaction dynamics. Consider a 2-player\n",
        "Kuhn Poker (turn-based game). Agents have access to the `observations` (a dict)\n",
        "field from `TimeSpec`, containing the following members:\n",
        " * `info_state`: list containing the game information state for each player. The\n",
        "   size of the list always correspond to the number of players. E.g.:\n",
        "   [[0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]].\n",
        " * `legal_actions`: list containing legal action ID lists (one for each player).\n",
        "   E.g.: [[0, 1], [0]], which corresponds to actions 0 and 1 being valid for\n",
        "   player 0 (the 1st player) and action 0 being valid for player 1 (2nd player).\n",
        " * `current_player`: zero-based integer representing the player to make a move.\n",
        "\n",
        "At each `step` call, the environment expects a singleton list with the action\n",
        "(as it's a turn-based game), e.g.: [1]. This (zero-based) action must correspond\n",
        "to the player specified at `current_player`. The game (which is at decision\n",
        "node) will process the action and take as many steps necessary to cover chance\n",
        "nodes, halting at a new decision or final node. Finally, a new `TimeStep`is\n",
        "returned to the agent.\n",
        "\n",
        "Simultaneous-move games follow analogous dynamics. The only differences is the\n",
        "environment expects a list of actions, one per player. Note the `current_player`\n",
        "field is \"irrelevant\" here, admitting a constant value defined in spiel.h, which\n",
        "defaults to -2 (module level constant `SIMULTANEOUS_PLAYER_ID`).\n",
        "\n",
        "See open_spiel/python/examples/rl_example.py for example usages.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "\n",
        "import enum\n",
        "from absl import logging\n",
        "import numpy as np\n",
        "\n",
        "import pyspiel\n",
        "\n",
        "SIMULTANEOUS_PLAYER_ID = pyspiel.PlayerId.SIMULTANEOUS\n",
        "\n",
        "class TimeStep(\n",
        "    collections.namedtuple(\n",
        "        \"TimeStep\", [\"observations\", \"rewards\", \"discounts\", \"step_type\"])):\n",
        "  \"\"\"Returned with every call to `step` and `reset`.\n",
        "\n",
        "  A `TimeStep` contains the data emitted by a game at each step of interaction.\n",
        "  A `TimeStep` holds an `observation` (list of dicts, one per player),\n",
        "  associated lists of `rewards`, `discounts` and a `step_type`.\n",
        "\n",
        "  The first `TimeStep` in a sequence will have `StepType.FIRST`. The final\n",
        "  `TimeStep` will have `StepType.LAST`. All other `TimeStep`s in a sequence will\n",
        "  have `StepType.MID.\n",
        "\n",
        "  Attributes:\n",
        "    observations: a list of dicts containing observations per player.\n",
        "    rewards: A list of scalars (one per player), or `None` if `step_type` is\n",
        "      `StepType.FIRST`, i.e. at the start of a sequence.\n",
        "    discounts: A list of discount values in the range `[0, 1]` (one per player),\n",
        "      or `None` if `step_type` is `StepType.FIRST`.\n",
        "    step_type: A `StepType` enum value.\n",
        "  \"\"\"\n",
        "  __slots__ = ()\n",
        "\n",
        "  def first(self):\n",
        "    return self.step_type == StepType.FIRST\n",
        "\n",
        "  def mid(self):\n",
        "    return self.step_type == StepType.MID\n",
        "\n",
        "  def last(self):\n",
        "    return self.step_type == StepType.LAST\n",
        "\n",
        "  def is_simultaneous_move(self):\n",
        "    return self.observations[\"current_player\"] == SIMULTANEOUS_PLAYER_ID\n",
        "\n",
        "  def current_player(self):\n",
        "    return self.observations[\"current_player\"]\n",
        "\n",
        "\n",
        "class StepType(enum.Enum):\n",
        "  \"\"\"Defines the status of a `TimeStep` within a sequence.\"\"\"\n",
        "\n",
        "  FIRST = 0  # Denotes the first `TimeStep` in a sequence.\n",
        "  MID = 1  # Denotes any `TimeStep` in a sequence that is not FIRST or LAST.\n",
        "  LAST = 2  # Denotes the last `TimeStep` in a sequence.\n",
        "\n",
        "  def first(self):\n",
        "    return self is StepType.FIRST\n",
        "\n",
        "  def mid(self):\n",
        "    return self is StepType.MID\n",
        "\n",
        "  def last(self):\n",
        "    return self is StepType.LAST\n",
        "\n",
        "\n",
        "# Global pyspiel members\n",
        "def registered_games():\n",
        "  return pyspiel.registered_games()\n",
        "\n",
        "\n",
        "\n",
        "class ChanceEventSampler(object):\n",
        "  \"\"\"Default sampler for external chance events.\"\"\"\n",
        "\n",
        "  def __init__(self, seed=None):\n",
        "    self.seed(seed)\n",
        "\n",
        "  def seed(self, seed=None):\n",
        "    self._rng = np.random.RandomState(seed)\n",
        "\n",
        "  def __call__(self, state):\n",
        "    \"\"\"Sample a chance event in the given state.\"\"\"\n",
        "    actions, probs = zip(*state.chance_outcomes())\n",
        "    return self._rng.choice(actions, p=probs)\n",
        "\n",
        "\n",
        "class ObservationType(enum.Enum):\n",
        "  \"\"\"Defines what kind of observation to use.\"\"\"\n",
        "  OBSERVATION = 0  # Use observation_tensor\n",
        "  INFORMATION_STATE = 1  # Use information_state_tensor\n",
        "\n",
        "\n",
        "class Environment(object):\n",
        "  \"\"\"Open Spiel reinforcement learning environment class.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               game,\n",
        "               discount=1.0,\n",
        "               chance_event_sampler=None,\n",
        "               observation_type=None,\n",
        "               include_full_state=False,\n",
        "               distribution=None,\n",
        "               mfg_population=None,\n",
        "               enable_legality_check=False,\n",
        "               **kwargs):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      game: [string, pyspiel.Game] Open Spiel game name or game instance.\n",
        "      discount: float, discount used in non-initial steps. Defaults to 1.0.\n",
        "      chance_event_sampler: optional object with `sample_external_events` method\n",
        "        to sample chance events.\n",
        "      observation_type: what kind of observation to use. If not specified, will\n",
        "        default to INFORMATION_STATE unless the game doesn't provide it.\n",
        "      include_full_state: whether or not to include the full serialized\n",
        "        OpenSpiel state in the observations (sometimes useful for debugging).\n",
        "      distribution: the distribution over states if the game is a mean field\n",
        "        game.\n",
        "      mfg_population: The Mean Field Game population to consider.\n",
        "      enable_legality_check: Check the legality of the move before stepping.\n",
        "      **kwargs: dict, additional settings passed to the Open Spiel game.\n",
        "    \"\"\"\n",
        "    self._chance_event_sampler = chance_event_sampler or ChanceEventSampler()\n",
        "    self._include_full_state = include_full_state\n",
        "    self._distribution = distribution\n",
        "    self._mfg_population = mfg_population\n",
        "    self._enable_legality_check = enable_legality_check\n",
        "\n",
        "    if isinstance(game, str):\n",
        "      if kwargs:\n",
        "        game_settings = {key: val for (key, val) in kwargs.items()}\n",
        "        logging.info(\"Using game settings: %s\", game_settings)\n",
        "        self._game = pyspiel.load_game(game, game_settings)\n",
        "      else:\n",
        "        logging.info(\"Using game string: %s\", game)\n",
        "        self._game = pyspiel.load_game(game)\n",
        "    else:  # pyspiel.Game or API-compatible object.\n",
        "      logging.info(\"Using game instance: %s\", game.get_type().short_name)\n",
        "      self._game = game\n",
        "\n",
        "    self._num_players = self._game.num_players()\n",
        "    self._state = None\n",
        "    self._should_reset = True\n",
        "\n",
        "    # Discount returned at non-initial  steps.\n",
        "    self._discounts = [discount] * self._num_players\n",
        "\n",
        "    # Determine what observation type to use.\n",
        "    if observation_type is None:\n",
        "      if self._game.get_type().provides_information_state_tensor:\n",
        "        observation_type = ObservationType.INFORMATION_STATE\n",
        "      else:\n",
        "        observation_type = ObservationType.OBSERVATION\n",
        "\n",
        "    # Check the requested observation type is supported.\n",
        "    if observation_type == ObservationType.OBSERVATION:\n",
        "      if not self._game.get_type().provides_observation_tensor:\n",
        "        raise ValueError(f\"observation_tensor not supported by {game}\")\n",
        "    elif observation_type == ObservationType.INFORMATION_STATE:\n",
        "      if not self._game.get_type().provides_information_state_tensor:\n",
        "        raise ValueError(f\"information_state_tensor not supported by {game}\")\n",
        "    self._use_observation = (observation_type == ObservationType.OBSERVATION)\n",
        "\n",
        "    if self._game.get_type().dynamics == pyspiel.GameType.Dynamics.MEAN_FIELD:\n",
        "      assert distribution is not None\n",
        "      assert mfg_population is not None\n",
        "      assert 0 <= mfg_population < self._num_players\n",
        "\n",
        "  def seed(self, seed=None):\n",
        "    self._chance_event_sampler.seed(seed)\n",
        "\n",
        "  def get_time_step(self):\n",
        "    \"\"\"Returns a `TimeStep` without updating the environment.\n",
        "\n",
        "    Returns:\n",
        "      A `TimeStep` namedtuple containing:\n",
        "        observation: list of dicts containing one observations per player, each\n",
        "          corresponding to `observation_spec()`.\n",
        "        reward: list of rewards at this timestep, or None if step_type is\n",
        "          `StepType.FIRST`.\n",
        "        discount: list of discounts in the range [0, 1], or None if step_type is\n",
        "          `StepType.FIRST`.\n",
        "        step_type: A `StepType` value.\n",
        "    \"\"\"\n",
        "    observations = {\n",
        "        \"info_state\": [],\n",
        "        \"edge_index\":[],\n",
        "        \"legal_actions\": [],\n",
        "        \"current_player\": [],\n",
        "        \"serialized_state\": []\n",
        "    }\n",
        "    rewards = []\n",
        "    step_type = StepType.LAST if self._state.is_terminal() else StepType.MID\n",
        "    self._should_reset = step_type == StepType.LAST\n",
        "    cur_rewards = self._state.rewards()\n",
        "    player_id = 0\n",
        "    rewards.append(cur_rewards[player_id])\n",
        "    observations[\"info_state\"].append(\n",
        "          self._state.observation_tensor(player_id) if self._use_observation\n",
        "          else self._state.information_state_tensor(player_id))\n",
        "    observations[\"legal_actions\"].append(self._state.legal_actions(player_id))\n",
        "    observations[\"edge_index\"]=self._state.edge_index\n",
        "    observations[\"current_player\"] = self._state.current_player()\n",
        "    discounts = self._discounts\n",
        "    if step_type == StepType.LAST:\n",
        "      # When the game is in a terminal state set the discount to 0.\n",
        "      discounts = [0. for _ in discounts]\n",
        "\n",
        "    if self._include_full_state:\n",
        "      observations[\"serialized_state\"] = pyspiel.serialize_game_and_state(\n",
        "          self._game, self._state)\n",
        "\n",
        "    return TimeStep(\n",
        "        observations=observations,\n",
        "        rewards=rewards,\n",
        "        discounts=discounts,\n",
        "        step_type=step_type)\n",
        "\n",
        "  def _check_legality(self, actions):\n",
        "    if self.is_turn_based:\n",
        "      legal_actions = self._state.legal_actions()\n",
        "      if actions[0] not in legal_actions:\n",
        "        raise RuntimeError(f\"step() called on illegal action {actions[0]}\")\n",
        "    else:\n",
        "      for p in range(len(actions)):\n",
        "        legal_actions = self._state.legal_actions(p)\n",
        "        if legal_actions and actions[p] not in legal_actions:\n",
        "          raise RuntimeError(f\"step() by player {p} called on illegal \" +\n",
        "                             f\"action: {actions[p]}\")\n",
        "\n",
        "  def step(self, actions):\n",
        "    \"\"\"Updates the environment according to `actions` and returns a `TimeStep`.\n",
        "\n",
        "    If the environment returned a `TimeStep` with `StepType.LAST` at the\n",
        "    previous step, this call to `step` will start a new sequence and `actions`\n",
        "    will be ignored.\n",
        "\n",
        "    This method will also start a new sequence if called after the environment\n",
        "    has been constructed and `reset` has not been called. Again, in this case\n",
        "    `actions` will be ignored.\n",
        "\n",
        "    Args:\n",
        "      actions: a list containing one action per player, following specifications\n",
        "        defined in `action_spec()`.\n",
        "\n",
        "    Returns:\n",
        "      A `TimeStep` namedtuple containing:\n",
        "        observation: list of dicts containing one observations per player, each\n",
        "          corresponding to `observation_spec()`.\n",
        "        reward: list of rewards at this timestep, or None if step_type is\n",
        "          `StepType.FIRST`.\n",
        "        discount: list of discounts in the range [0, 1], or None if step_type is\n",
        "          `StepType.FIRST`.\n",
        "        step_type: A `StepType` value.\n",
        "    \"\"\"\n",
        "    assert len(actions) == self.num_actions_per_step, (\n",
        "        \"Invalid number of actions! Expected {}\".format(\n",
        "            self.num_actions_per_step))\n",
        "    if self._should_reset:\n",
        "      return self.reset()\n",
        "\n",
        "    if self._enable_legality_check:\n",
        "      self._check_legality(actions)\n",
        "\n",
        "    if self.is_turn_based:\n",
        "      self._state.apply_action(actions[0])\n",
        "    else:\n",
        "      self._state.apply_actions(actions)\n",
        "\n",
        "\n",
        "    return self.get_time_step()\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Starts a new sequence and returns the first `TimeStep` of this sequence.\n",
        "\n",
        "    Returns:\n",
        "      A `TimeStep` namedtuple containing:\n",
        "        observations: list of dicts containing one observations per player, each\n",
        "          corresponding to `observation_spec()`.\n",
        "        rewards: list of rewards at this timestep, or None if step_type is\n",
        "          `StepType.FIRST`.\n",
        "        discounts: list of discounts in the range [0, 1], or None if step_type\n",
        "          is `StepType.FIRST`.\n",
        "        step_type: A `StepType` value.\n",
        "    \"\"\"\n",
        "    self._should_reset = False\n",
        "    if self._game.get_type(\n",
        "    ).dynamics == pyspiel.GameType.Dynamics.MEAN_FIELD and self._num_players > 1:\n",
        "      self._state = self._game.new_initial_state_for_population(\n",
        "          self._mfg_population)\n",
        "    else:\n",
        "      self._state = self._game.new_initial_state()\n",
        "\n",
        "    observations = {\n",
        "        \"info_state\": [],\n",
        "        \"edge_index\":[],\n",
        "        \"legal_actions\": [],\n",
        "        \"current_player\": [],\n",
        "        \"serialized_state\": []\n",
        "    }\n",
        "    player_id = 0\n",
        "    observations[\"info_state\"].append(\n",
        "        self._state.observation_tensor(player_id) if self._use_observation\n",
        "        else self._state.information_state_tensor(player_id))\n",
        "    observations[\"legal_actions\"].append(self._state.legal_actions(player_id))\n",
        "    observations[\"edge_index\"]=self._state.edge_index\n",
        "    observations[\"current_player\"] = self._state.current_player()\n",
        "\n",
        "\n",
        "    if self._include_full_state:\n",
        "      observations[\"serialized_state\"] = pyspiel.serialize_game_and_state(\n",
        "          self._game, self._state)\n",
        "\n",
        "    return TimeStep(\n",
        "        observations=observations,\n",
        "        rewards=None,\n",
        "        discounts=None,\n",
        "        step_type=StepType.FIRST)\n",
        "\n",
        "\n",
        "  def observation_spec(self):\n",
        "    \"\"\"Defines the observation per player provided by the environment.\n",
        "\n",
        "    Each dict member will contain its expected structure and shape. E.g.: for\n",
        "    Kuhn Poker {\"info_state\": (6,), \"legal_actions\": (2,), \"current_player\": (),\n",
        "                \"serialized_state\": ()}\n",
        "\n",
        "    Returns:\n",
        "      A specification dict describing the observation fields and shapes.\n",
        "    \"\"\"\n",
        "    return dict(\n",
        "        info_state=tuple([\n",
        "            self._game.observation_tensor if self._use_observation else\n",
        "            self._game.information_state_tensor_size\n",
        "        ]),\n",
        "        edge_index=(),\n",
        "        legal_actions=(self._game.num_distinct_actions(),),\n",
        "        current_player=(),\n",
        "        serialized_state=(),\n",
        "    )\n",
        "\n",
        "  def action_spec(self):\n",
        "    \"\"\"Defines per player action specifications.\n",
        "\n",
        "    Specifications include action boundaries and their data type.\n",
        "    E.g.: for Kuhn Poker {\"num_actions\": 2, \"min\": 0, \"max\":1, \"dtype\": int}\n",
        "\n",
        "    Returns:\n",
        "      A specification dict containing per player action properties.\n",
        "    \"\"\"\n",
        "    return dict(\n",
        "        num_actions=self._game.num_distinct_actions(),\n",
        "        min=0,\n",
        "        max=self._game.num_distinct_actions() - 1,\n",
        "        dtype=int,\n",
        "    )\n",
        "\n",
        "  # Environment properties\n",
        "  @property\n",
        "  def use_observation(self):\n",
        "    \"\"\"Returns whether the environment is using the game's observation.\n",
        "\n",
        "    If false, it is using the game's information state.\n",
        "    \"\"\"\n",
        "    return self._use_observation\n",
        "\n",
        "  # Game properties\n",
        "  @property\n",
        "  def name(self):\n",
        "    return self._game.get_type().short_name\n",
        "\n",
        "  @property\n",
        "  def num_players(self):\n",
        "    return self._game.num_players()\n",
        "\n",
        "  @property\n",
        "  def num_actions_per_step(self):\n",
        "    return 10 #set as number of actions\n",
        "\n",
        "  # New RL calls for more advanced use cases (e.g. search + RL).\n",
        "  @property\n",
        "  def is_turn_based(self):\n",
        "    return ((self._game.get_type().dynamics\n",
        "             == pyspiel.GameType.Dynamics.SEQUENTIAL) or\n",
        "            (self._game.get_type().dynamics\n",
        "             == pyspiel.GameType.Dynamics.MEAN_FIELD))\n",
        "\n",
        "  @property\n",
        "  def max_game_length(self):\n",
        "    return self._game.max_game_length()\n",
        "\n",
        "  @property\n",
        "  def is_chance_node(self):\n",
        "    return self._state.is_chance_node()\n",
        "\n",
        "  @property\n",
        "  def game(self):\n",
        "    return self._game\n",
        "\n",
        "  def set_state(self, new_state):\n",
        "    \"\"\"Updates the game state.\"\"\"\n",
        "    assert new_state.get_game() == self.game, (\n",
        "        \"State must have been created by the same game.\")\n",
        "    self._state = new_state\n",
        "\n",
        "  @property\n",
        "  def get_state(self):\n",
        "    return self._state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmICdphwIOXx"
      },
      "source": [
        "# Game"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 532,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gen_graph(cur_n, g_type):\n",
        "    if g_type == 'erdos_renyi':\n",
        "        g = nx.erdos_renyi_graph(n=cur_n, p=0.15)\n",
        "    elif g_type == 'powerlaw':\n",
        "        g = nx.powerlaw_cluster_graph(n=cur_n, m=4, p=0.05)\n",
        "    elif g_type == 'small-world':\n",
        "        g = nx.connected_watts_strogatz_graph(n=cur_n, k=8, p=0.1)\n",
        "    elif g_type == 'barabasi_albert':\n",
        "        g = nx.barabasi_albert_graph(n=cur_n, m=4)\n",
        "    return g\n",
        "\n",
        "def gen_new_graphs(number_nodes):\n",
        "    print('\\ngenerating new training graphs...')\n",
        "    graph_type = ['erdos_renyi', 'powerlaw','small-world', 'barabasi_albert']\n",
        "    a = np.random.choice(graph_type)\n",
        "    graph = gen_graph(number_nodes, a)\n",
        "    #graph = gen_graph(number_nodes, 'small-world')\n",
        "    active = 1\n",
        "    nx.set_node_attributes(graph,active, \"active\")\n",
        "    index = {n: {\"index\":n} for n in np.arange(len(graph))}\n",
        "    nx.set_node_attributes(graph,index)\n",
        "    return graph    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 527,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "generating new training graphs...\n"
          ]
        }
      ],
      "source": [
        "def gen_new_graphs(number_nodes):\n",
        "    print('\\ngenerating new training graphs...')\n",
        "    graph_type = ['erdos_renyi', 'powerlaw','small-world', 'barabasi_albert']\n",
        "    a = np.random.choice(graph_type)\n",
        "    graph = gen_graph(number_nodes, a)\n",
        "    #graph = gen_graph(number_nodes, 'small-world')\n",
        "    active = 1\n",
        "    nx.set_node_attributes(graph,active, \"active\")\n",
        "    index = {n: {\"index\":n} for n in np.arange(len(graph))}\n",
        "    nx.set_node_attributes(graph,index)\n",
        "    return graph   \n",
        "nx.write_edgelist(gen_new_graphs(50), \"./real/test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 528,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n"
          ]
        }
      ],
      "source": [
        "#Re-register the Environment \n",
        "\n",
        "#GRAPH = nx.read_edgelist(\"./real/corruption.txt\")\n",
        "GRAPH = nx.read_edgelist(\"./real/test\")\n",
        "_NUM_PLAYERS = 2\n",
        "_NUM_CELLS = len(GRAPH)  \n",
        "print(_NUM_CELLS)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import random\n",
        "import copy\n",
        "import networkx as nx\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric import utils\n",
        "from open_spiel.python.observation import IIGObserverForPublicInfoGame\n",
        "import pyspiel\n",
        "from operator import itemgetter\n",
        "\n",
        "\n",
        "\n",
        "def reset(graph):\n",
        "    active = int(1)\n",
        "    nx.set_node_attributes(graph,active, \"active\")\n",
        "    index = {int(n): {\"index\":int(n)} for n in np.arange(len(graph))}\n",
        "    nx.set_node_attributes(graph,index)\n",
        "    return graph   \n",
        "\n",
        "\n",
        "class GraphGame(pyspiel.Game):\n",
        "  \"\"\"A Python version of the Graph game.\"\"\"\n",
        "\n",
        "  def __init__(self, params=None):\n",
        "    super().__init__(_GAME_TYPE, _GAME_INFO, params or dict())\n",
        "\n",
        "  def new_initial_state(self):\n",
        "    \"\"\"Returns a state corresponding to the start of a game.\"\"\"\n",
        "    return GraphState(self)\n",
        "\n",
        "  def make_py_observer(self, iig_obs_type=None, params=None):\n",
        "    \"\"\"Returns an object used for observing game state.\"\"\"\n",
        "    '''\n",
        "    if ((iig_obs_type is None) or\n",
        "        (iig_obs_type.public_info and not iig_obs_type.perfect_recall)):\n",
        "      return BoardObserver(params)\n",
        "    else:\n",
        "      return IIGObserverForPublicInfoGame(iig_obs_type, params)\n",
        "    '''\n",
        "    return BoardObserver(params)\n",
        "\n",
        "\n",
        "class GraphState(pyspiel.State):\n",
        "  \"\"\"A python version of the Tic-Tac-Toe state.\"\"\"\n",
        "  def __init__(self, game):\n",
        "    \"\"\"Constructor; should only be called by Game.new_initial_state.\"\"\"\n",
        "    super().__init__(game)\n",
        "    self._is_terminal = False\n",
        "    #self.board = reset(nx.read_edgelist(\"./real/corruption.txt\"))\n",
        "    self.board = reset(GRAPH)\n",
        "    self.num_nodes = len(self.board)\n",
        "    self.num_feature = 3\n",
        "    self.edge_index = utils.convert.from_networkx(self.board).edge_index\n",
        "    self._rewards = np.zeros(_NUM_PLAYERS)\n",
        "    self._returns = np.zeros(_NUM_PLAYERS)\n",
        "    self.lcc = [1]\n",
        "    self.r = []\n",
        "    self.timestep = 1\n",
        "\n",
        "  # OpenSpiel (PySpiel) API functions are below. This is the standard set that\n",
        "  # should be implemented by every perfect-information sequential-move game.\n",
        "\n",
        "  def current_player(self):\n",
        "    \"\"\"Returns id of the next player to move, or TERMINAL if game is over.\"\"\"\n",
        "    #return pyspiel.PlayerId.TERMINAL if self._is_terminal else pyspiel.PlayerId.SIMULTANEOUS\n",
        "    return pyspiel.PlayerId.TERMINAL if self._is_terminal else pyspiel.PlayerId.SIMULTANEOUS\n",
        "  \n",
        "\n",
        "  def _legal_actions(self, player):\n",
        "    \"\"\"Returns a list of legal actions, sorted in ascending order.\"\"\"\n",
        "    all_nodes = np.array(list(self.board.nodes(data=\"active\")),dtype=int)[:,1]\n",
        "    action_sequence = np.where(all_nodes == 1)[0]\n",
        "    return action_sequence\n",
        "\n",
        "  def _apply_actions(self, actions):\n",
        "    \"\"\"Applies the specified action to the state.\"\"\"\n",
        "    self.r.append(self._rewards[0])\n",
        "    actions = np.array(actions,dtype=np.dtype('<U21'))\n",
        "    for action in actions:\n",
        "      self.board.nodes[action][\"active\"] = '0'\n",
        "    _, cond, l = _network_dismantle(self.board)\n",
        "    #self.data = features(subgraph)\n",
        "    decrease_lcc = (self.lcc[-1] - l)\n",
        "    self._rewards[0] = decrease_lcc*10\n",
        "    self._rewards[1] = -self._rewards[0]\n",
        "    self._returns += self._rewards\n",
        "    self.lcc.append(l)\n",
        "    if self.timestep==5:\n",
        "        #cond = True \n",
        "        None\n",
        "    else: \n",
        "        self.timestep+=1\n",
        "    self._is_terminal = cond\n",
        "\n",
        "\n",
        "  def _action_to_string(self, player, action):\n",
        "    \"\"\"Action -> string.\"\"\"\n",
        "    return \"{}({})\".format(0 if player == 0 else 1, action)\n",
        "\n",
        "  def is_terminal(self):\n",
        "    \"\"\"Returns True if the game is over.\"\"\"\n",
        "    return self._is_terminal\n",
        "\n",
        "  def returns(self):\n",
        "    \"\"\"Total reward for each player over the course of the game so far.\"\"\"\n",
        "    return self._returns\n",
        "  def rewards(self):\n",
        "    \"\"\"Total reward for each player over the course of the game so far.\"\"\"\n",
        "    return self._rewards\n",
        "\n",
        "  def __str__(self):\n",
        "    \"\"\"String for debug purposes. No particular semantics are required.\"\"\"\n",
        "    return _board_to_string(self.board)\n",
        "\n",
        "  def new_initial_state(self):\n",
        "    self.edge_index = utils.convert.from_networkx(self.board).edge_index\n",
        "    self.board = reset(self.board)\n",
        "\n",
        "class BoardObserver:\n",
        "  \"\"\"Observer, conforming to the PyObserver interface (see observation.py).\"\"\"\n",
        "\n",
        "  def __init__(self,params):\n",
        "    \"\"\"Initializes an empty observation tensor.\"\"\"\n",
        "    if params:\n",
        "      raise ValueError(f\"Observation parameters not supported; passed {params}\")\n",
        "    # The observation should contain a 1-D tensor in `self.t ensor` and a\n",
        "    # dictionary of views onto the tensor, which may be of any shape.\n",
        "    # Here the observation is indexed `(cell state, row, column)\n",
        "    _NUM_FEATURES = 3\n",
        "    shape = ( _NUM_CELLS,_NUM_FEATURES)\n",
        "    self.tensor = np.zeros(np.prod(shape), np.float32)\n",
        "    self.dict = {\"observation\":np.reshape(self.tensor, shape)}\n",
        "    #self.dict = {\"observation\":self.tensor}\n",
        "\n",
        "\n",
        "  def set_from(self, state, player):\n",
        "    \"\"\"Updates `tensor` and `dict` to reflect `state` from PoV of `player`.\"\"\"\n",
        "    # We update the observation via the shaped tensor since indexing is more\n",
        "    # convenient than with the 1-D tensor. Both are views onto the same memory.\n",
        "    obs = self.dict[\"observation\"]\n",
        "    obs.fill(0)\n",
        "    all_nodes = np.array(list(state.board.nodes(data=\"active\")))[:,1]\n",
        "    attack_nodes = np.array(np.where(all_nodes == '1')[0],dtype=str)\n",
        "    subGraph = state.board.subgraph(np.array(attack_nodes,dtype=np.dtype('<U21')))\n",
        "    data = np.array(features(subGraph))\n",
        "    for i, x in enumerate(subGraph.nodes()):\n",
        "        (i)\n",
        "        obs[i,:] = data[i,:]\n",
        "    self.tensor =obs.flatten()\n",
        "    return self.tensor\n",
        "\n",
        "  def string_from(self, state, player):\n",
        "    \"\"\"Observation of `state` from the PoV of `player`, as a string.\"\"\"\n",
        "    return _board_to_string(state.board)\n",
        "\n",
        "\n",
        "# Helper functions for game details.\n",
        "def get_index(g,index):\n",
        "    node_location = nx.get_node_attributes(g, \"index\")\n",
        "    return [node_location.get(key) for key in index]\n",
        "\n",
        "def features(g):\n",
        "    degree_centrality = list(nx.degree_centrality(g).values())\n",
        "    #precolation_centrality = list(nx.percolation_centrality(g).values())\n",
        "    #closeness_centrality = list(nx.closeness_centrality(g).values())\n",
        "    #eigen_centrality = list(nx.eigenvector_centrality(g,tol=1e-04).values())\n",
        "    clustering_coeff = list(nx.clustering(g).values())\n",
        "    core_num = list(nx.core_number(g).values())\n",
        "    #pagerank = list(nx.pagerank(g).values())\n",
        "    #x = np.column_stack((degree_centrality,clustering_coeff,pagerank, core_num ))\n",
        "    x = np.column_stack((degree_centrality,clustering_coeff, core_num ))\n",
        "    return x\n",
        "\n",
        "def _network_dismantle(board):\n",
        "    \"\"\"Checks if a line exists, returns \"x\" or \"o\" if so, and None otherwise.\"\"\"\n",
        "    all_nodes = np.array(list(board.nodes(data=\"active\")))[:,1]\n",
        "    alive_nodes = np.array(np.where(all_nodes == '1')[0],dtype=str)\n",
        "    subGraph = board.subgraph(np.array(alive_nodes,dtype=np.dtype('<U21')))\n",
        "    largest_cc = len(max(nx.connected_components(subGraph), key=len))/len(board)\n",
        "    cond = True if largest_cc <= 0.1 else False\n",
        "    cond = True if len(alive_nodes) <= 10 or len(alive_nodes) <= 10 else False\n",
        "    return subGraph, cond, largest_cc\n",
        "\n",
        "\n",
        "def _board_to_string(board):\n",
        "    \"\"\"Returns a string representation of the board.\"\"\"\n",
        "    value = np.array(list(board.nodes(data=\"active\")))\n",
        "    return \" \".join(str(board.nodes[e][\"index\"])+\"{\"+str(f)+\"}\" for e, f in value)\n",
        "_GAME_TYPE = pyspiel.GameType(\n",
        "    short_name=\"graph_attack_defend\",\n",
        "    long_name=\"Python Attack Defend\",\n",
        "    dynamics=pyspiel.GameType.Dynamics.SIMULTANEOUS,\n",
        "    #dynamics=pyspiel.GameType.Dynamics.SEQUENTIAL,\n",
        "    chance_mode=pyspiel.GameType.ChanceMode.EXPLICIT_STOCHASTIC,\n",
        "    information=pyspiel.GameType.Information.IMPERFECT_INFORMATION,\n",
        "    utility=pyspiel.GameType.Utility.ZERO_SUM,\n",
        "    reward_model=pyspiel.GameType.RewardModel.REWARDS,\n",
        "    max_num_players=1,\n",
        "    min_num_players=1,\n",
        "    provides_information_state_string=True,\n",
        "    provides_information_state_tensor=True,\n",
        "    provides_observation_string=False,\n",
        "    provides_observation_tensor=False,\n",
        "    provides_factored_observation_string=True)\n",
        "\n",
        "_GAME_INFO = pyspiel.GameInfo(\n",
        "    num_distinct_actions=10,\n",
        "    max_chance_outcomes=0,\n",
        "    num_players=1,\n",
        "    min_utility=-1.0,\n",
        "    max_utility=1.0,\n",
        "    utility_sum=0.0,\n",
        "    max_game_length=_NUM_CELLS)\n",
        "pyspiel.register_game(_GAME_TYPE, GraphGame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 529,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Got it from DQN class to get the probabilities for the best action\n",
        "from open_spiel.python import rl_agent\n",
        "def step(agent,time_step):\n",
        "    \"\"\"Returns the action to be taken and updates the Q-network if needed.\n",
        "    Args:\n",
        "      time_step: an instance of rl_environment.TimeStep.\n",
        "      is_evaluation: bool, whether this is a training or evaluation call.\n",
        "      add_transition_record: Whether to add to the replay buffer on this step.\n",
        "    Returns:\n",
        "      A `rl_agent.StepOutput` containing the action probs and chosen action.\n",
        "    \"\"\"\n",
        "\n",
        "    # Act step: don't act at terminal info states or if its not our turn.\n",
        "    index = torch.Tensor(np.arange(10))\n",
        "    if (not time_step.last()) and (\n",
        "        time_step.is_simultaneous_move() or\n",
        "        agent.player_id == time_step.current_player()):\n",
        "      info_state = time_step.observations[\"info_state\"][agent.player_id]\n",
        "      legal_actions = np.array(time_step.observations[\"legal_actions\"][agent.player_id])\n",
        "      edge_index = time_step.observations[\"edge_index\"]\n",
        "      probs = np.zeros(agent._num_actions)\n",
        "      #get values in terms of (Data infostate, Edgelist)\n",
        "      shape = (int(len(info_state)/agent.num_feature),agent.num_feature)\n",
        "      x = np.reshape(info_state, shape)\n",
        "      data = torch.from_numpy(x.astype(np.float32))\n",
        "      print(agent._q_network)\n",
        "      q_values = agent._q_network(data.type(torch.FloatTensor),edge_index.type(torch.LongTensor)).detach()\n",
        "      legal_q_values = q_values[legal_actions]\n",
        "      sorted = torch.sort(legal_q_values,descending=True,stable=True)\n",
        "      print(q_values)\n",
        "      if torch.sum(torch.eq(sorted[0],torch.ones(agent._num_actions)))!= (agent._num_actions-10):\n",
        "         action = np.random.randint(agent._num_actions,size=10)\n",
        "         #action = torch.index_select(legal_actions,1,torch.rand(10))\n",
        "      else:\n",
        "        action = sorted[1][:10]\n",
        "      probs[action] = 1.0\n",
        "    return rl_agent.StepOutput(action=action, probs=probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 530,
      "metadata": {},
      "outputs": [],
      "source": [
        "def large(env, attacker):\n",
        "    \"\"\"Evaluates `attacker` against a new graph.\"\"\"\n",
        "    time_step = env.reset()\n",
        "    episode_rewards = []\n",
        "    i = 0\n",
        "    while not time_step.last():\n",
        "        agents_output = step(attacker,time_step)\n",
        "        action_list = agents_output.action\n",
        "        time_step = env.step(action_list)\n",
        "        #drawNetwork(env.get_state.board,str(i))\n",
        "        i+=1\n",
        "        episode_rewards.append(env.get_state._rewards[0])\n",
        "    episode_rewards,lcc = env.get_state.lcc\n",
        "    # build gif\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 531,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GraphNN(\n",
            "  (conv1): GATv2Conv(3, 64, heads=1)\n",
            "  (conv2): GATv2Conv(64, 1, heads=1)\n",
            ")\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'GraphNN' object has no attribute 'conv3'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/bhandk/Desktop/MAS/open_spiel_simultaneous_DQN_GAT_Test.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bhandk/Desktop/MAS/open_spiel_simultaneous_DQN_GAT_Test.ipynb#ch0000028?line=7'>8</a>\u001b[0m agent \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mdqn_test0\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bhandk/Desktop/MAS/open_spiel_simultaneous_DQN_GAT_Test.ipynb#ch0000028?line=8'>9</a>\u001b[0m agent\u001b[39m.\u001b[39m_num_actions \u001b[39m=\u001b[39m _NUM_CELLS\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bhandk/Desktop/MAS/open_spiel_simultaneous_DQN_GAT_Test.ipynb#ch0000028?line=9'>10</a>\u001b[0m rewards, lcc, action_lists \u001b[39m=\u001b[39m large(env, agent)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bhandk/Desktop/MAS/open_spiel_simultaneous_DQN_GAT_Test.ipynb#ch0000028?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(rewards)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bhandk/Desktop/MAS/open_spiel_simultaneous_DQN_GAT_Test.ipynb#ch0000028?line=11'>12</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(lcc)\n",
            "\u001b[1;32m/home/bhandk/Desktop/MAS/open_spiel_simultaneous_DQN_GAT_Test.ipynb Cell 14'\u001b[0m in \u001b[0;36mlarge\u001b[0;34m(env, attacker)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bhandk/Desktop/MAS/open_spiel_simultaneous_DQN_GAT_Test.ipynb#ch0000027?line=4'>5</a>\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bhandk/Desktop/MAS/open_spiel_simultaneous_DQN_GAT_Test.ipynb#ch0000027?line=5'>6</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m time_step\u001b[39m.\u001b[39mlast():\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/bhandk/Desktop/MAS/open_spiel_simultaneous_DQN_GAT_Test.ipynb#ch0000027?line=6'>7</a>\u001b[0m     agents_output \u001b[39m=\u001b[39m step(attacker,time_step)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bhandk/Desktop/MAS/open_spiel_simultaneous_DQN_GAT_Test.ipynb#ch0000027?line=7'>8</a>\u001b[0m     action_list \u001b[39m=\u001b[39m agents_output\u001b[39m.\u001b[39maction\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bhandk/Desktop/MAS/open_spiel_simultaneous_DQN_GAT_Test.ipynb#ch0000027?line=8'>9</a>\u001b[0m     time_step \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action_list)\n",
            "\u001b[1;32m/home/bhandk/Desktop/MAS/open_spiel_simultaneous_DQN_GAT_Test.ipynb Cell 13'\u001b[0m in \u001b[0;36mstep\u001b[0;34m(agent, time_step)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bhandk/Desktop/MAS/open_spiel_simultaneous_DQN_GAT_Test.ipynb#ch0000041?line=24'>25</a>\u001b[0m data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(x\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bhandk/Desktop/MAS/open_spiel_simultaneous_DQN_GAT_Test.ipynb#ch0000041?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(agent\u001b[39m.\u001b[39m_q_network)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bhandk/Desktop/MAS/open_spiel_simultaneous_DQN_GAT_Test.ipynb#ch0000041?line=26'>27</a>\u001b[0m q_values \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49m_q_network(data\u001b[39m.\u001b[39;49mtype(torch\u001b[39m.\u001b[39;49mFloatTensor),edge_index\u001b[39m.\u001b[39;49mtype(torch\u001b[39m.\u001b[39;49mLongTensor))\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bhandk/Desktop/MAS/open_spiel_simultaneous_DQN_GAT_Test.ipynb#ch0000041?line=27'>28</a>\u001b[0m legal_q_values \u001b[39m=\u001b[39m q_values[legal_actions]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bhandk/Desktop/MAS/open_spiel_simultaneous_DQN_GAT_Test.ipynb#ch0000041?line=28'>29</a>\u001b[0m \u001b[39msorted\u001b[39m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msort(legal_q_values,descending\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,stable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mGraphNN.forward\u001b[0;34m(self, node_feature, edge_index)\u001b[0m\n\u001b[1;32m     65\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x, edge_index))\n\u001b[1;32m     66\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m---> 67\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv3(x, edge_index)\n\u001b[1;32m     68\u001b[0m \u001b[39m#x = F.linear(x,size,bias=False)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(x)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1177\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1174'>1175</a>\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1175'>1176</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1176'>1177</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   <a href='file:///~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1177'>1178</a>\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GraphNN' object has no attribute 'conv3'"
          ]
        }
      ],
      "source": [
        "#Create new environment\n",
        "game = \"graph_attack_defend\"\n",
        "num_players = 1\n",
        "env = Environment(game)\n",
        "info_state_size = env.observation_spec()[\"info_state\"][0]\n",
        "feature_size = 3\n",
        "num_actions = 20 #env.action_spec()[\"num_actions\"]\n",
        "agent = torch.load('dqn_test0')\n",
        "agent._num_actions = _NUM_CELLS\n",
        "rewards, lcc, action_lists = large(env, agent)\n",
        "print(rewards)\n",
        "plt.plot(lcc)\n",
        "plt.title(\"(Real World)LCC/n vs iteration\")\n",
        "plt.savefig(\"(Real World)LCC/n vs iteration\")\n",
        "plt.show()\n",
        "plt.clf()\n",
        "plt.plot(rewards)\n",
        "plt.title(\"Rewards for Attacker\")\n",
        "plt.savefig(\"(Real World)Rewards\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "open_spiel_simultaneous_DQN_GNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
