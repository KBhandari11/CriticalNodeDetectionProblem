{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOOzDGYAZcW3"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/cpu/torch_stable.html\n",
            "Requirement already satisfied: torch==1.11.0+cpu in /home/bhandk/miniconda3/lib/python3.8/site-packages (1.11.0+cpu)\n",
            "Requirement already satisfied: torchvision==0.12.0+cpu in /home/bhandk/miniconda3/lib/python3.8/site-packages (0.12.0+cpu)\n",
            "Requirement already satisfied: torchaudio==0.11.0+cpu in /home/bhandk/miniconda3/lib/python3.8/site-packages (0.11.0+cpu)\n",
            "Requirement already satisfied: typing-extensions in /home/bhandk/miniconda3/lib/python3.8/site-packages (from torch==1.11.0+cpu) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /home/bhandk/miniconda3/lib/python3.8/site-packages (from torchvision==0.12.0+cpu) (1.22.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from torchvision==0.12.0+cpu) (9.0.1)\n",
            "Requirement already satisfied: requests in /home/bhandk/miniconda3/lib/python3.8/site-packages (from torchvision==0.12.0+cpu) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from requests->torchvision==0.12.0+cpu) (1.26.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from requests->torchvision==0.12.0+cpu) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from requests->torchvision==0.12.0+cpu) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from requests->torchvision==0.12.0+cpu) (2.0.4)\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.11.0+cpu.html\n",
            "Requirement already satisfied: torch-scatter in /home/bhandk/miniconda3/lib/python3.8/site-packages (2.0.9)\n",
            "Requirement already satisfied: torch-sparse in /home/bhandk/miniconda3/lib/python3.8/site-packages (0.6.13)\n",
            "Requirement already satisfied: torch-cluster in /home/bhandk/miniconda3/lib/python3.8/site-packages (1.6.0)\n",
            "Requirement already satisfied: torch-spline-conv in /home/bhandk/miniconda3/lib/python3.8/site-packages (1.2.1)\n",
            "Requirement already satisfied: torch-geometric in /home/bhandk/miniconda3/lib/python3.8/site-packages (2.0.4)\n",
            "Requirement already satisfied: scipy in /home/bhandk/miniconda3/lib/python3.8/site-packages (from torch-sparse) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: scikit-learn in /home/bhandk/miniconda3/lib/python3.8/site-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: requests in /home/bhandk/miniconda3/lib/python3.8/site-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /home/bhandk/miniconda3/lib/python3.8/site-packages (from torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: numpy in /home/bhandk/miniconda3/lib/python3.8/site-packages (from torch-geometric) (1.22.3)\n",
            "Requirement already satisfied: tqdm in /home/bhandk/miniconda3/lib/python3.8/site-packages (from torch-geometric) (4.63.0)\n",
            "Requirement already satisfied: pandas in /home/bhandk/miniconda3/lib/python3.8/site-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from pandas->torch-geometric) (2021.3)\n",
            "Requirement already satisfied: six>=1.5 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->torch-geometric) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from requests->torch-geometric) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from requests->torch-geometric) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from requests->torch-geometric) (1.26.8)\n",
            "Requirement already satisfied: joblib>=0.11 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: open_spiel in /home/bhandk/miniconda3/lib/python3.8/site-packages (1.1.0)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from open_spiel) (21.4.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from open_spiel) (1.22.3)\n",
            "Requirement already satisfied: pip>=20.0.2 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from open_spiel) (21.2.4)\n",
            "Requirement already satisfied: scipy>=1.5.4 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from open_spiel) (1.8.0)\n",
            "Requirement already satisfied: absl-py>=0.10.0 in /home/bhandk/miniconda3/lib/python3.8/site-packages (from open_spiel) (1.0.0)\n",
            "Requirement already satisfied: six in /home/bhandk/miniconda3/lib/python3.8/site-packages (from absl-py>=0.10.0->open_spiel) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.11.0+cpu torchvision==0.12.0+cpu torchaudio==0.11.0+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.11.0+cpu.html\n",
        "!pip install --upgrade open_spiel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cKmGDQKGLE4S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘figure’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir figure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npECI_8fgzuh"
      },
      "source": [
        "# Game Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJSK2n6cbHwq",
        "outputId": "980c159f-f57f-4f54-f47c-c5352cd1c54d"
      },
      "outputs": [],
      "source": [
        "# Copyright 2019 DeepMind Technologies Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "# Lint as python3\n",
        "\"\"\" Graph Attack and Defense implemented in Python.\n",
        "This is a simple demonstration of implementing a game in Python, featuring\n",
        "chance and imperfect information.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import random \n",
        "import copy\n",
        "import networkx as nx\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric import utils\n",
        "from open_spiel.python.observation import IIGObserverForPublicInfoGame\n",
        "import pyspiel\n",
        "from operator import itemgetter\n",
        "\n",
        "\n",
        "def gen_graph(cur_n, g_type):\n",
        "    if g_type == 'erdos_renyi':\n",
        "        g = nx.erdos_renyi_graph(n=cur_n, p=random.uniform(0.05,0.25))\n",
        "    elif g_type == 'powerlaw':\n",
        "        g = nx.powerlaw_cluster_graph(n=cur_n, m=random.randint(2,4), p=random.uniform(0.01,0.1))\n",
        "    elif g_type == 'small-world':\n",
        "        g = nx.connected_watts_strogatz_graph(n=cur_n, k=random.randint(5,10), p=random.uniform(0.01,0.2))\n",
        "    elif g_type == 'barabasi_albert':\n",
        "        g = nx.barabasi_albert_graph(n=cur_n, m=random.randint(3,5))\n",
        "    return g\n",
        "\n",
        "def gen_new_graphs():\n",
        "    graph_type = ['erdos_renyi', 'powerlaw','small-world', 'barabasi_albert']\n",
        "    a = np.random.choice(graph_type)\n",
        "    number_nodes = 50#random.randint(30,50)\n",
        "    graph = gen_graph(number_nodes, a)\n",
        "    active = 1\n",
        "    nx.set_node_attributes(graph,active, \"active\")\n",
        "    return graph    \n",
        "def reset(graph):\n",
        "    active = 1\n",
        "    nx.set_node_attributes(graph,active, \"active\")\n",
        "    return graph   \n",
        "\n",
        "\n",
        "class GraphGame(pyspiel.Game):\n",
        "  \"\"\"A Python version of the Graph game.\"\"\"\n",
        "\n",
        "  def __init__(self, params=None):\n",
        "    super().__init__(_GAME_TYPE, _GAME_INFO, params or dict())\n",
        "\n",
        "  def new_initial_state(self):\n",
        "    \"\"\"Returns a state corresponding to the start of a game.\"\"\"\n",
        "    return GraphState(self)\n",
        "\n",
        "  def make_py_observer(self, iig_obs_type=None, params=None):\n",
        "    \"\"\"Returns an object used for observing game state.\"\"\"\n",
        "    '''\n",
        "    if ((iig_obs_type is None) or\n",
        "        (iig_obs_type.public_info and not iig_obs_type.perfect_recall)):\n",
        "      return BoardObserver(params)\n",
        "    else:\n",
        "      return IIGObserverForPublicInfoGame(iig_obs_type, params)\n",
        "    '''\n",
        "    return BoardObserver(params)\n",
        "\n",
        "\n",
        "class GraphState(pyspiel.State):\n",
        "  \"\"\"A python version of the Tic-Tac-Toe state.\"\"\"\n",
        "  def __init__(self, game):\n",
        "    \"\"\"Constructor; should only be called by Game.new_initial_state.\"\"\"\n",
        "    super().__init__(game)\n",
        "    self._is_terminal = False\n",
        "    self.board = gen_new_graphs()\n",
        "    self.num_nodes = len(self.board)\n",
        "    self.num_feature = 5\n",
        "    self.x = features(self.board)\n",
        "    self.subGraph = self.board.copy()\n",
        "    self.init_lcc = len(max(nx.connected_components(self.board), key=len))\n",
        "    self._rewards = np.zeros(_NUM_PLAYERS)\n",
        "    self._returns = np.zeros(_NUM_PLAYERS)\n",
        "    self.lcc = [1]\n",
        "    self.r = []\n",
        "    self.limit = 1/self.num_nodes\n",
        "\n",
        "  # OpenSpiel (PySpiel) API functions are below. This is the standard set that\n",
        "  # should be implemented by every perfect-information sequential-move game.\n",
        "\n",
        "  def current_player(self):\n",
        "    \"\"\"Returns id of the next player to move, or TERMINAL if game is over.\"\"\"\n",
        "    #return pyspiel.PlayerId.TERMINAL if self._is_terminal else pyspiel.PlayerId.SIMULTANEOUS\n",
        "    return pyspiel.PlayerId.TERMINAL if self._is_terminal else pyspiel.PlayerId.SIMULTANEOUS\n",
        "  \n",
        "\n",
        "  def _legal_actions(self, player):\n",
        "    \"\"\"Returns a list of legal actions, sorted in ascending order.\"\"\"\n",
        "    all_nodes = np.array(list(self.board.nodes(data=\"active\")))[:,1]\n",
        "    active_nodes = np.where(all_nodes == 1)[0]\n",
        "    if player == 0 :\n",
        "        action_sequence = np.squeeze(np.append(active_nodes,np.where(all_nodes == 3)))\n",
        "    elif player == 1:\n",
        "        action_sequence = active_nodes \n",
        "    else:\n",
        "        action_sequence =  active_nodes\n",
        "    return action_sequence\n",
        "\n",
        "  def _apply_actions(self, actions):\n",
        "    \"\"\"Applies the specified action to the state.\"\"\"\n",
        "    self.r.append(self._rewards[0])\n",
        "    #attack_node = self.board.nodes[actions[0]][\"index\"]\n",
        "    attack_node = actions[0]\n",
        "    #defend_node = self.board.nodes[actions[1]][\"index\"]\n",
        "    defend_node = actions[1]\n",
        "    if (actions[0] == actions[1]):\n",
        "        self.board.nodes[attack_node][\"active\"] = 0\n",
        "    else: \n",
        "        self.board.nodes[attack_node][\"active\"] = 0\n",
        "        self.board.nodes[defend_node][\"active\"] = 2\n",
        "    _, cond, l = _network_dismantle(self.board, self.init_lcc)\n",
        "    ebunch = list(self.subGraph.edges(attack_node))\n",
        "    self.subGraph.remove_edges_from(ebunch)\n",
        "    self.x = features(self.subGraph)\n",
        "    decrease_lcc = (self.lcc[-1] - l)\n",
        "    if decrease_lcc >= 4*self.limit:\n",
        "      self._rewards[0] = 1 #decrease_lcc*10\n",
        "    else:\n",
        "      self._rewards[0] = 0\n",
        "    self._rewards[1] = -self._rewards[0]\n",
        "    self._returns += self._rewards\n",
        "    self.lcc.append(l)\n",
        "    self._is_terminal = cond\n",
        "\n",
        "\n",
        "  def _action_to_string(self, player, action):\n",
        "    \"\"\"Action -> string.\"\"\"\n",
        "    return \"{}({})\".format(0 if player == 0 else 1, action)\n",
        "\n",
        "  def is_terminal(self):\n",
        "    \"\"\"Returns True if the game is over.\"\"\"\n",
        "    return self._is_terminal\n",
        "\n",
        "  def returns(self):\n",
        "    \"\"\"Total reward for each player over the course of the game so far.\"\"\"\n",
        "    return self._returns\n",
        "  def rewards(self):\n",
        "    \"\"\"Total reward for each player over the course of the game so far.\"\"\"\n",
        "    return self._rewards\n",
        "\n",
        "  def __str__(self):\n",
        "    \"\"\"String for debug purposes. No particular semantics are required.\"\"\"\n",
        "    return _board_to_string(self.board)\n",
        "\n",
        "  def new_initial_state(self):\n",
        "      #self.board = reset(self.board)\n",
        "      self.board = gen_new_graphs()\n",
        "      self.x = features(self.board)\n",
        "\n",
        "\n",
        "class BoardObserver:\n",
        "  \"\"\"Observer, conforming to the PyObserver interface (see observation.py).\"\"\"\n",
        "\n",
        "  def __init__(self,params):\n",
        "    \"\"\"Initializes an empty observation tensor.\"\"\"\n",
        "    if params:\n",
        "      raise ValueError(f\"Observation parameters not supported; passed {params}\")\n",
        "    # The observation should contain a 1-D tensor in `self.tensor` and a\n",
        "    # dictionary of views onto the tensor, which may be of any shape.\n",
        "    # Here the observation is indexed `(cell state, row, column)\n",
        "    self.tensor = np.zeros(50*50)#np.array([])\n",
        "    self.dict = {\"observation\":self.tensor}\n",
        "    #self.dict = {\"observation\":self.tensor}\n",
        "\n",
        "\n",
        "  def set_from(self, state, player):\n",
        "    \"\"\"Updates `tensor` and `dict` to reflect `state` from PoV of `player`.\"\"\"\n",
        "    # We update the observation via the shaped tensor since indexing is more\n",
        "    # convenient than with the 1-D tensor. Both are views onto the same memory.\n",
        "    obs = self.dict[\"observation\"]\n",
        "    obs = np.zeros((state.num_nodes))\n",
        "    '''all_nodes = np.array(list(state.board.nodes(data=\"active\")))[:,1]\n",
        "    attacked_nodes = np.where(all_nodes == 0)[0]\n",
        "    alive_nodes = np.array(list(set(state.board.nodes).difference(set(attacked_nodes))))'''\n",
        "    adj = np.ravel(nx.to_numpy_array(state.subGraph))\n",
        "    for i,x in enumerate(adj):\n",
        "        obs[i] = x\n",
        "    self.tensor =obs.flatten()\n",
        "    return self.tensor\n",
        "\n",
        "  def string_from(self, state, player):\n",
        "    \"\"\"Observation of `state` from the PoV of `player`, as a string.\"\"\"\n",
        "    return _board_to_string(state.board)\n",
        "\n",
        "\n",
        "# Helper functions for game details.\n",
        "\n",
        "def features(g):\n",
        "    degree_centrality = list(nx.degree_centrality(g).values())\n",
        "    #precolation_centrality = list(nx.percolation_centrality(g).values())\n",
        "    #closeness_centrality = list(nx.closeness_centrality(g).values())\n",
        "    eigen_centrality = list(nx.eigenvector_centrality(g,tol=1e-03).values())\n",
        "    clustering_coeff = list(nx.clustering(g).values())\n",
        "    core_num = list(nx.core_number(g).values())\n",
        "    pagerank = list(nx.pagerank(g).values())\n",
        "    #x = np.column_stack((degree_centrality,clustering_coeff,pagerank, core_num ))\n",
        "    x = np.column_stack((degree_centrality,eigen_centrality,pagerank,clustering_coeff, core_num ))\n",
        "    return x\n",
        "\n",
        "def _network_dismantle(board, init_lcc):\n",
        "    \"\"\"Checks if a line exists, returns \"x\" or \"o\" if so, and None otherwise.\"\"\"\n",
        "    all_nodes =  np.array(list(board.nodes(data=\"active\")))[:,1]\n",
        "    active_nodes =  np.where(all_nodes == 1)[0]\n",
        "    attacked_nodes =  np.where(all_nodes == 0)[0]\n",
        "    alive_nodes = set(board.nodes).difference(set(attacked_nodes))\n",
        "    acc = board.subgraph(alive_nodes)\n",
        "    largest_cc = len(max(nx.connected_components(acc), key=len))/init_lcc\n",
        "    cond = True if (len(alive_nodes) <= 2 or len(active_nodes) <= 2) or largest_cc <= 0.3 else False\n",
        "    return attacked_nodes, cond, largest_cc\n",
        "\n",
        "\n",
        "def _board_to_string(board):\n",
        "    \"\"\"Returns a string representation of the board.\"\"\"\n",
        "    value = np.array(list(board.nodes(data=\"active\")))\n",
        "    return \" \".join(str(f) for e, f in value)\n",
        "\n",
        "# Register the game with the OpenSpiel library\n",
        "\n",
        "_NUM_PLAYERS = 2\n",
        "_MAX_CELLS = 50\n",
        "GRAPH = gen_new_graphs()\n",
        "#nx.write_adjlist(GRAPH, \"/content/figure/Graph\")\n",
        "\n",
        "_GAME_TYPE = pyspiel.GameType(\n",
        "    short_name=\"graph_attack_defend\",\n",
        "    long_name=\"Python Attack Defend\",\n",
        "    dynamics=pyspiel.GameType.Dynamics.SIMULTANEOUS,\n",
        "    chance_mode=pyspiel.GameType.ChanceMode.EXPLICIT_STOCHASTIC,\n",
        "    information=pyspiel.GameType.Information.IMPERFECT_INFORMATION,\n",
        "    utility=pyspiel.GameType.Utility.ZERO_SUM,\n",
        "    reward_model=pyspiel.GameType.RewardModel.REWARDS,\n",
        "    max_num_players=_NUM_PLAYERS,\n",
        "    min_num_players=_NUM_PLAYERS,\n",
        "    provides_information_state_string=True,\n",
        "    provides_information_state_tensor=True,\n",
        "    provides_observation_string=False,\n",
        "    provides_observation_tensor=False,\n",
        "    provides_factored_observation_string=True)\n",
        "\n",
        "_GAME_INFO = pyspiel.GameInfo(\n",
        "    num_distinct_actions=_MAX_CELLS,\n",
        "    max_chance_outcomes=0,\n",
        "    num_players=2,\n",
        "    min_utility=-1.0,\n",
        "    max_utility=1.0,\n",
        "    utility_sum=0.0,\n",
        "    max_game_length=_MAX_CELLS)\n",
        "pyspiel.register_game(_GAME_TYPE, GraphGame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhlYXvFSfC09"
      },
      "source": [
        "# Testing the Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8X9RzUZubQJ"
      },
      "source": [
        "## General PlayThrough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvIxsHeSua8p",
        "outputId": "08a1f92a-9146-4c5f-f9d3-8bce37cd32bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['game: graph_attack_defend',\n",
              " '',\n",
              " 'GameType.chance_mode = ChanceMode.EXPLICIT_STOCHASTIC',\n",
              " 'GameType.dynamics = Dynamics.SIMULTANEOUS',\n",
              " 'GameType.information = Information.IMPERFECT_INFORMATION',\n",
              " 'GameType.long_name = \"Python Attack Defend\"',\n",
              " 'GameType.max_num_players = 2',\n",
              " 'GameType.min_num_players = 2',\n",
              " 'GameType.parameter_specification = []',\n",
              " 'GameType.provides_information_state_string = True',\n",
              " 'GameType.provides_information_state_tensor = True',\n",
              " 'GameType.provides_observation_string = False',\n",
              " 'GameType.provides_observation_tensor = False',\n",
              " 'GameType.provides_factored_observation_string = True',\n",
              " 'GameType.reward_model = RewardModel.REWARDS',\n",
              " 'GameType.short_name = \"graph_attack_defend\"',\n",
              " 'GameType.utility = Utility.ZERO_SUM',\n",
              " '',\n",
              " 'NumDistinctActions() = 50',\n",
              " 'PolicyTensorShape() = [50]',\n",
              " 'MaxChanceOutcomes() = 0',\n",
              " 'GetParameters() = {}',\n",
              " 'NumPlayers() = 2',\n",
              " 'MinUtility() = -1.0',\n",
              " 'MaxUtility() = 1.0',\n",
              " 'UtilitySum() = 0.0',\n",
              " 'InformationStateTensorShape() = [2500]',\n",
              " 'InformationStateTensorLayout() = TensorLayout.CHW',\n",
              " 'InformationStateTensorSize() = 2500',\n",
              " 'ObservationTensorShape() = [2500]',\n",
              " 'ObservationTensorLayout() = TensorLayout.CHW',\n",
              " 'ObservationTensorSize() = 2500',\n",
              " 'MaxGameLength() = 50',\n",
              " 'ToString() = \"graph_attack_defend()\"',\n",
              " '',\n",
              " '# State 0',\n",
              " '# 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1',\n",
              " 'IsTerminal() = False',\n",
              " 'History() = []',\n",
              " 'HistoryString() = \"\"',\n",
              " 'IsChanceNode() = False',\n",
              " 'IsSimultaneousNode() = True',\n",
              " 'CurrentPlayer() = PlayerId.SIMULTANEOUS',\n",
              " 'InformationStateString(0) = \"1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'InformationStateString(1) = \"1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'InformationStateTensor(0).observation: zeros(2500)',\n",
              " 'InformationStateTensor(1).observation: zeros(2500)',\n",
              " 'ObservationString(0) = \"1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'ObservationString(1) = \"1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'PublicObservationString() = \"1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'PrivateObservationString(0) = \"1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'PrivateObservationString(1) = \"1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'ObservationTensor(0): zeros(2500)',\n",
              " 'ObservationTensor(1): zeros(2500)',\n",
              " 'Rewards() = [0. 0.]',\n",
              " 'Returns() = [0. 0.]',\n",
              " 'LegalActions(0) = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]',\n",
              " 'LegalActions(1) = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]',\n",
              " 'StringLegalActions(0) = [\"0(0)\", \"0(1)\", \"0(2)\", \"0(3)\", \"0(4)\", \"0(5)\", \"0(6)\", \"0(7)\", \"0(8)\", \"0(9)\", \"0(10)\", \"0(11)\", \"0(12)\", \"0(13)\", \"0(14)\", \"0(15)\", \"0(16)\", \"0(17)\", \"0(18)\", \"0(19)\", \"0(20)\", \"0(21)\", \"0(22)\", \"0(23)\", \"0(24)\", \"0(25)\", \"0(26)\", \"0(27)\", \"0(28)\", \"0(29)\", \"0(30)\", \"0(31)\", \"0(32)\", \"0(33)\", \"0(34)\", \"0(35)\", \"0(36)\", \"0(37)\", \"0(38)\", \"0(39)\", \"0(40)\", \"0(41)\", \"0(42)\", \"0(43)\", \"0(44)\", \"0(45)\", \"0(46)\", \"0(47)\", \"0(48)\", \"0(49)\"]',\n",
              " 'StringLegalActions(1) = [\"1(0)\", \"1(1)\", \"1(2)\", \"1(3)\", \"1(4)\", \"1(5)\", \"1(6)\", \"1(7)\", \"1(8)\", \"1(9)\", \"1(10)\", \"1(11)\", \"1(12)\", \"1(13)\", \"1(14)\", \"1(15)\", \"1(16)\", \"1(17)\", \"1(18)\", \"1(19)\", \"1(20)\", \"1(21)\", \"1(22)\", \"1(23)\", \"1(24)\", \"1(25)\", \"1(26)\", \"1(27)\", \"1(28)\", \"1(29)\", \"1(30)\", \"1(31)\", \"1(32)\", \"1(33)\", \"1(34)\", \"1(35)\", \"1(36)\", \"1(37)\", \"1(38)\", \"1(39)\", \"1(40)\", \"1(41)\", \"1(42)\", \"1(43)\", \"1(44)\", \"1(45)\", \"1(46)\", \"1(47)\", \"1(48)\", \"1(49)\"]',\n",
              " '',\n",
              " '# Apply joint action [\"0(4)\", \"1(19)\"]',\n",
              " 'actions: [4, 19]',\n",
              " '',\n",
              " '# State 1',\n",
              " '# 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1',\n",
              " 'IsTerminal() = False',\n",
              " 'History() = [4, 19]',\n",
              " 'HistoryString() = \"4, 19\"',\n",
              " 'IsChanceNode() = False',\n",
              " 'IsSimultaneousNode() = True',\n",
              " 'CurrentPlayer() = PlayerId.SIMULTANEOUS',\n",
              " 'InformationStateString(0) = \"1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'InformationStateString(1) = \"1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'InformationStateTensor(0).observation: zeros(2500)',\n",
              " 'InformationStateTensor(1).observation: zeros(2500)',\n",
              " 'ObservationString(0) = \"1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'ObservationString(1) = \"1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'PublicObservationString() = \"1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'PrivateObservationString(0) = \"1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'PrivateObservationString(1) = \"1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'ObservationTensor(0): zeros(2500)',\n",
              " 'ObservationTensor(1): zeros(2500)',\n",
              " 'Rewards() = [ 0. -0.]',\n",
              " 'Returns() = [0. 0.]',\n",
              " 'LegalActions(0) = [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]',\n",
              " 'LegalActions(1) = [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]',\n",
              " 'StringLegalActions(0) = [\"0(0)\", \"0(1)\", \"0(2)\", \"0(3)\", \"0(5)\", \"0(6)\", \"0(7)\", \"0(8)\", \"0(9)\", \"0(10)\", \"0(11)\", \"0(12)\", \"0(13)\", \"0(14)\", \"0(15)\", \"0(16)\", \"0(17)\", \"0(18)\", \"0(20)\", \"0(21)\", \"0(22)\", \"0(23)\", \"0(24)\", \"0(25)\", \"0(26)\", \"0(27)\", \"0(28)\", \"0(29)\", \"0(30)\", \"0(31)\", \"0(32)\", \"0(33)\", \"0(34)\", \"0(35)\", \"0(36)\", \"0(37)\", \"0(38)\", \"0(39)\", \"0(40)\", \"0(41)\", \"0(42)\", \"0(43)\", \"0(44)\", \"0(45)\", \"0(46)\", \"0(47)\", \"0(48)\", \"0(49)\"]',\n",
              " 'StringLegalActions(1) = [\"1(0)\", \"1(1)\", \"1(2)\", \"1(3)\", \"1(5)\", \"1(6)\", \"1(7)\", \"1(8)\", \"1(9)\", \"1(10)\", \"1(11)\", \"1(12)\", \"1(13)\", \"1(14)\", \"1(15)\", \"1(16)\", \"1(17)\", \"1(18)\", \"1(20)\", \"1(21)\", \"1(22)\", \"1(23)\", \"1(24)\", \"1(25)\", \"1(26)\", \"1(27)\", \"1(28)\", \"1(29)\", \"1(30)\", \"1(31)\", \"1(32)\", \"1(33)\", \"1(34)\", \"1(35)\", \"1(36)\", \"1(37)\", \"1(38)\", \"1(39)\", \"1(40)\", \"1(41)\", \"1(42)\", \"1(43)\", \"1(44)\", \"1(45)\", \"1(46)\", \"1(47)\", \"1(48)\", \"1(49)\"]',\n",
              " '',\n",
              " '# Apply joint action [\"0(36)\", \"1(34)\"]',\n",
              " 'actions: [36, 34]',\n",
              " '',\n",
              " '# State 2',\n",
              " '# 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1',\n",
              " 'IsTerminal() = False',\n",
              " 'History() = [4, 19, 36, 34]',\n",
              " 'HistoryString() = \"4, 19, 36, 34\"',\n",
              " 'IsChanceNode() = False',\n",
              " 'IsSimultaneousNode() = True',\n",
              " 'CurrentPlayer() = PlayerId.SIMULTANEOUS',\n",
              " 'InformationStateString(0) = \"1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'InformationStateString(1) = \"1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'InformationStateTensor(0).observation: zeros(2500)',\n",
              " 'InformationStateTensor(1).observation: zeros(2500)',\n",
              " 'ObservationString(0) = \"1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'ObservationString(1) = \"1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'PublicObservationString() = \"1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'PrivateObservationString(0) = \"1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'PrivateObservationString(1) = \"1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\"',\n",
              " 'ObservationTensor(0): zeros(2500)',\n",
              " 'ObservationTensor(1): zeros(2500)',\n",
              " 'Rewards() = [ 0. -0.]',\n",
              " 'Returns() = [0. 0.]',\n",
              " 'LegalActions(0) = [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]',\n",
              " 'LegalActions(1) = [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]',\n",
              " 'StringLegalActions(0) = [\"0(0)\", \"0(1)\", \"0(2)\", \"0(3)\", \"0(5)\", \"0(6)\", \"0(7)\", \"0(8)\", \"0(9)\", \"0(10)\", \"0(11)\", \"0(12)\", \"0(13)\", \"0(14)\", \"0(15)\", \"0(16)\", \"0(17)\", \"0(18)\", \"0(20)\", \"0(21)\", \"0(22)\", \"0(23)\", \"0(24)\", \"0(25)\", \"0(26)\", \"0(27)\", \"0(28)\", \"0(29)\", \"0(30)\", \"0(31)\", \"0(32)\", \"0(33)\", \"0(35)\", \"0(37)\", \"0(38)\", \"0(39)\", \"0(40)\", \"0(41)\", \"0(42)\", \"0(43)\", \"0(44)\", \"0(45)\", \"0(46)\", \"0(47)\", \"0(48)\", \"0(49)\"]',\n",
              " 'StringLegalActions(1) = [\"1(0)\", \"1(1)\", \"1(2)\", \"1(3)\", \"1(5)\", \"1(6)\", \"1(7)\", \"1(8)\", \"1(9)\", \"1(10)\", \"1(11)\", \"1(12)\", \"1(13)\", \"1(14)\", \"1(15)\", \"1(16)\", \"1(17)\", \"1(18)\", \"1(20)\", \"1(21)\", \"1(22)\", \"1(23)\", \"1(24)\", \"1(25)\", \"1(26)\", \"1(27)\", \"1(28)\", \"1(29)\", \"1(30)\", \"1(31)\", \"1(32)\", \"1(33)\", \"1(35)\", \"1(37)\", \"1(38)\", \"1(39)\", \"1(40)\", \"1(41)\", \"1(42)\", \"1(43)\", \"1(44)\", \"1(45)\", \"1(46)\", \"1(47)\", \"1(48)\", \"1(49)\"]',\n",
              " '',\n",
              " '# Apply joint action [\"0(7)\", \"1(8)\"]',\n",
              " 'actions: [7, 8]',\n",
              " '',\n",
              " '# State 3',\n",
              " '# Apply joint action [\"0(10)\", \"1(28)\"]',\n",
              " 'actions: [10, 28]',\n",
              " '',\n",
              " '# State 4',\n",
              " '# Apply joint action [\"0(40)\", \"1(15)\"]',\n",
              " 'actions: [40, 15]',\n",
              " '',\n",
              " '# State 5',\n",
              " '# Apply joint action [\"0(17)\", \"1(20)\"]',\n",
              " 'actions: [17, 20]',\n",
              " '',\n",
              " '# State 6',\n",
              " '# Apply joint action [\"0(27)\", \"1(27)\"]',\n",
              " 'actions: [27, 27]',\n",
              " '',\n",
              " '# State 7',\n",
              " '# Apply joint action [\"0(49)\", \"1(45)\"]',\n",
              " 'actions: [49, 45]',\n",
              " '',\n",
              " '# State 8',\n",
              " '# Apply joint action [\"0(23)\", \"1(24)\"]',\n",
              " 'actions: [23, 24]',\n",
              " '',\n",
              " '# State 9',\n",
              " '# Apply joint action [\"0(0)\", \"1(37)\"]',\n",
              " 'actions: [0, 37]',\n",
              " '',\n",
              " '# State 10',\n",
              " '# 0 1 1 1 0 1 1 0 2 1 0 1 1 1 1 2 1 0 1 2 2 1 1 0 2 1 1 0 2 1 1 1 1 1 2 1 0 2 1 1 0 1 1 1 1 2 1 1 1 0',\n",
              " 'IsTerminal() = False',\n",
              " 'History() = [4, 19, 36, 34, 7, 8, 10, 28, 40, 15, 17, 20, 27, 27, 49, 45, 23, 24, 0, 37]',\n",
              " 'HistoryString() = \"4, 19, 36, 34, 7, 8, 10, 28, 40, 15, 17, 20, 27, 27, 49, 45, 23, 24, 0, 37\"',\n",
              " 'IsChanceNode() = False',\n",
              " 'IsSimultaneousNode() = True',\n",
              " 'CurrentPlayer() = PlayerId.SIMULTANEOUS',\n",
              " 'InformationStateString(0) = \"0 1 1 1 0 1 1 0 2 1 0 1 1 1 1 2 1 0 1 2 2 1 1 0 2 1 1 0 2 1 1 1 1 1 2 1 0 2 1 1 0 1 1 1 1 2 1 1 1 0\"',\n",
              " 'InformationStateString(1) = \"0 1 1 1 0 1 1 0 2 1 0 1 1 1 1 2 1 0 1 2 2 1 1 0 2 1 1 0 2 1 1 1 1 1 2 1 0 2 1 1 0 1 1 1 1 2 1 1 1 0\"',\n",
              " 'InformationStateTensor(0).observation: zeros(2500)',\n",
              " 'InformationStateTensor(1).observation: zeros(2500)',\n",
              " 'ObservationString(0) = \"0 1 1 1 0 1 1 0 2 1 0 1 1 1 1 2 1 0 1 2 2 1 1 0 2 1 1 0 2 1 1 1 1 1 2 1 0 2 1 1 0 1 1 1 1 2 1 1 1 0\"',\n",
              " 'ObservationString(1) = \"0 1 1 1 0 1 1 0 2 1 0 1 1 1 1 2 1 0 1 2 2 1 1 0 2 1 1 0 2 1 1 1 1 1 2 1 0 2 1 1 0 1 1 1 1 2 1 1 1 0\"',\n",
              " 'PublicObservationString() = \"0 1 1 1 0 1 1 0 2 1 0 1 1 1 1 2 1 0 1 2 2 1 1 0 2 1 1 0 2 1 1 1 1 1 2 1 0 2 1 1 0 1 1 1 1 2 1 1 1 0\"',\n",
              " 'PrivateObservationString(0) = \"0 1 1 1 0 1 1 0 2 1 0 1 1 1 1 2 1 0 1 2 2 1 1 0 2 1 1 0 2 1 1 1 1 1 2 1 0 2 1 1 0 1 1 1 1 2 1 1 1 0\"',\n",
              " 'PrivateObservationString(1) = \"0 1 1 1 0 1 1 0 2 1 0 1 1 1 1 2 1 0 1 2 2 1 1 0 2 1 1 0 2 1 1 1 1 1 2 1 0 2 1 1 0 1 1 1 1 2 1 1 1 0\"',\n",
              " 'ObservationTensor(0): zeros(2500)',\n",
              " 'ObservationTensor(1): zeros(2500)',\n",
              " 'Rewards() = [ 0. -0.]',\n",
              " 'Returns() = [0. 0.]',\n",
              " 'LegalActions(0) = [1, 2, 3, 5, 6, 9, 11, 12, 13, 14, 16, 18, 21, 22, 25, 26, 29, 30, 31, 32, 33, 35, 38, 39, 41, 42, 43, 44, 46, 47, 48]',\n",
              " 'LegalActions(1) = [1, 2, 3, 5, 6, 9, 11, 12, 13, 14, 16, 18, 21, 22, 25, 26, 29, 30, 31, 32, 33, 35, 38, 39, 41, 42, 43, 44, 46, 47, 48]',\n",
              " 'StringLegalActions(0) = [\"0(1)\", \"0(2)\", \"0(3)\", \"0(5)\", \"0(6)\", \"0(9)\", \"0(11)\", \"0(12)\", \"0(13)\", \"0(14)\", \"0(16)\", \"0(18)\", \"0(21)\", \"0(22)\", \"0(25)\", \"0(26)\", \"0(29)\", \"0(30)\", \"0(31)\", \"0(32)\", \"0(33)\", \"0(35)\", \"0(38)\", \"0(39)\", \"0(41)\", \"0(42)\", \"0(43)\", \"0(44)\", \"0(46)\", \"0(47)\", \"0(48)\"]',\n",
              " 'StringLegalActions(1) = [\"1(1)\", \"1(2)\", \"1(3)\", \"1(5)\", \"1(6)\", \"1(9)\", \"1(11)\", \"1(12)\", \"1(13)\", \"1(14)\", \"1(16)\", \"1(18)\", \"1(21)\", \"1(22)\", \"1(25)\", \"1(26)\", \"1(29)\", \"1(30)\", \"1(31)\", \"1(32)\", \"1(33)\", \"1(35)\", \"1(38)\", \"1(39)\", \"1(41)\", \"1(42)\", \"1(43)\", \"1(44)\", \"1(46)\", \"1(47)\", \"1(48)\"]',\n",
              " '',\n",
              " '# Apply joint action [\"0(42)\", \"1(12)\"]',\n",
              " 'actions: [42, 12]',\n",
              " '',\n",
              " '# State 11',\n",
              " '# Apply joint action [\"0(21)\", \"1(31)\"]',\n",
              " 'actions: [21, 31]',\n",
              " '',\n",
              " '# State 12',\n",
              " '# Apply joint action [\"0(14)\", \"1(1)\"]',\n",
              " 'actions: [14, 1]',\n",
              " '',\n",
              " '# State 13',\n",
              " '# Apply joint action [\"0(44)\", \"1(9)\"]',\n",
              " 'actions: [44, 9]',\n",
              " '',\n",
              " '# State 14',\n",
              " '# Apply joint action [\"0(16)\", \"1(2)\"]',\n",
              " 'actions: [16, 2]',\n",
              " '',\n",
              " '# State 15',\n",
              " '# Apply joint action [\"0(11)\", \"1(30)\"]',\n",
              " 'actions: [11, 30]',\n",
              " '',\n",
              " '# State 16',\n",
              " '# Apply joint action [\"0(6)\", \"1(22)\"]',\n",
              " 'actions: [6, 22]',\n",
              " '',\n",
              " '# State 17',\n",
              " '# Apply joint action [\"0(47)\", \"1(43)\"]',\n",
              " 'actions: [47, 43]',\n",
              " '',\n",
              " '# State 18',\n",
              " '# Apply joint action [\"0(35)\", \"1(32)\"]',\n",
              " 'actions: [35, 32]',\n",
              " '',\n",
              " '# State 19',\n",
              " '# Apply joint action [\"0(39)\", \"1(25)\"]',\n",
              " 'actions: [39, 25]',\n",
              " '',\n",
              " '# State 20',\n",
              " '# 0 2 2 1 0 1 0 0 2 2 0 0 2 1 0 2 0 0 1 2 2 0 2 0 2 2 1 0 2 1 2 2 2 1 2 0 0 2 1 0 0 1 0 2 0 2 1 0 1 0',\n",
              " 'IsTerminal() = False',\n",
              " 'History() = [4, 19, 36, 34, 7, 8, 10, 28, 40, 15, 17, 20, 27, 27, 49, 45, 23, 24, 0, 37, 42, 12, 21, 31, 14, 1, 44, 9, 16, 2, 11, 30, 6, 22, 47, 43, 35, 32, 39, 25]',\n",
              " 'HistoryString() = \"4, 19, 36, 34, 7, 8, 10, 28, 40, 15, 17, 20, 27, 27, 49, 45, 23, 24, 0, 37, 42, 12, 21, 31, 14, 1, 44, 9, 16, 2, 11, 30, 6, 22, 47, 43, 35, 32, 39, 25\"',\n",
              " 'IsChanceNode() = False',\n",
              " 'IsSimultaneousNode() = True',\n",
              " 'CurrentPlayer() = PlayerId.SIMULTANEOUS',\n",
              " 'InformationStateString(0) = \"0 2 2 1 0 1 0 0 2 2 0 0 2 1 0 2 0 0 1 2 2 0 2 0 2 2 1 0 2 1 2 2 2 1 2 0 0 2 1 0 0 1 0 2 0 2 1 0 1 0\"',\n",
              " 'InformationStateString(1) = \"0 2 2 1 0 1 0 0 2 2 0 0 2 1 0 2 0 0 1 2 2 0 2 0 2 2 1 0 2 1 2 2 2 1 2 0 0 2 1 0 0 1 0 2 0 2 1 0 1 0\"',\n",
              " 'InformationStateTensor(0).observation: zeros(2500)',\n",
              " 'InformationStateTensor(1).observation: zeros(2500)',\n",
              " 'ObservationString(0) = \"0 2 2 1 0 1 0 0 2 2 0 0 2 1 0 2 0 0 1 2 2 0 2 0 2 2 1 0 2 1 2 2 2 1 2 0 0 2 1 0 0 1 0 2 0 2 1 0 1 0\"',\n",
              " 'ObservationString(1) = \"0 2 2 1 0 1 0 0 2 2 0 0 2 1 0 2 0 0 1 2 2 0 2 0 2 2 1 0 2 1 2 2 2 1 2 0 0 2 1 0 0 1 0 2 0 2 1 0 1 0\"',\n",
              " 'PublicObservationString() = \"0 2 2 1 0 1 0 0 2 2 0 0 2 1 0 2 0 0 1 2 2 0 2 0 2 2 1 0 2 1 2 2 2 1 2 0 0 2 1 0 0 1 0 2 0 2 1 0 1 0\"',\n",
              " 'PrivateObservationString(0) = \"0 2 2 1 0 1 0 0 2 2 0 0 2 1 0 2 0 0 1 2 2 0 2 0 2 2 1 0 2 1 2 2 2 1 2 0 0 2 1 0 0 1 0 2 0 2 1 0 1 0\"',\n",
              " 'PrivateObservationString(1) = \"0 2 2 1 0 1 0 0 2 2 0 0 2 1 0 2 0 0 1 2 2 0 2 0 2 2 1 0 2 1 2 2 2 1 2 0 0 2 1 0 0 1 0 2 0 2 1 0 1 0\"',\n",
              " 'ObservationTensor(0): zeros(2500)',\n",
              " 'ObservationTensor(1): zeros(2500)',\n",
              " 'Rewards() = [ 0. -0.]',\n",
              " 'Returns() = [0. 0.]',\n",
              " 'LegalActions(0) = [3, 5, 13, 18, 26, 29, 33, 38, 41, 46, 48]',\n",
              " 'LegalActions(1) = [3, 5, 13, 18, 26, 29, 33, 38, 41, 46, 48]',\n",
              " 'StringLegalActions(0) = [\"0(3)\", \"0(5)\", \"0(13)\", \"0(18)\", \"0(26)\", \"0(29)\", \"0(33)\", \"0(38)\", \"0(41)\", \"0(46)\", \"0(48)\"]',\n",
              " 'StringLegalActions(1) = [\"1(3)\", \"1(5)\", \"1(13)\", \"1(18)\", \"1(26)\", \"1(29)\", \"1(33)\", \"1(38)\", \"1(41)\", \"1(46)\", \"1(48)\"]',\n",
              " '',\n",
              " '# Apply joint action [\"0(13)\", \"1(29)\"]',\n",
              " 'actions: [13, 29]',\n",
              " '',\n",
              " '# State 21',\n",
              " '# Apply joint action [\"0(26)\", \"1(5)\"]',\n",
              " 'actions: [26, 5]',\n",
              " '',\n",
              " '# State 22',\n",
              " '# Apply joint action [\"0(41)\", \"1(18)\"]',\n",
              " 'actions: [41, 18]',\n",
              " '',\n",
              " '# State 23',\n",
              " '# Apply joint action [\"0(48)\", \"1(48)\"]',\n",
              " 'actions: [48, 48]',\n",
              " '',\n",
              " '# State 24',\n",
              " '# Apply joint action [\"0(46)\", \"1(3)\"]',\n",
              " 'actions: [46, 3]',\n",
              " '',\n",
              " '# State 25',\n",
              " '# 0 2 2 2 0 2 0 0 2 2 0 0 2 0 0 2 0 0 2 2 2 0 2 0 2 2 0 0 2 2 2 2 2 1 2 0 0 2 1 0 0 0 0 2 0 2 0 0 0 0',\n",
              " 'IsTerminal() = True',\n",
              " 'History() = [4, 19, 36, 34, 7, 8, 10, 28, 40, 15, 17, 20, 27, 27, 49, 45, 23, 24, 0, 37, 42, 12, 21, 31, 14, 1, 44, 9, 16, 2, 11, 30, 6, 22, 47, 43, 35, 32, 39, 25, 13, 29, 26, 5, 41, 18, 48, 48, 46, 3]',\n",
              " 'HistoryString() = \"4, 19, 36, 34, 7, 8, 10, 28, 40, 15, 17, 20, 27, 27, 49, 45, 23, 24, 0, 37, 42, 12, 21, 31, 14, 1, 44, 9, 16, 2, 11, 30, 6, 22, 47, 43, 35, 32, 39, 25, 13, 29, 26, 5, 41, 18, 48, 48, 46, 3\"',\n",
              " 'IsChanceNode() = False',\n",
              " 'IsSimultaneousNode() = False',\n",
              " 'CurrentPlayer() = PlayerId.TERMINAL',\n",
              " 'InformationStateString(0) = \"0 2 2 2 0 2 0 0 2 2 0 0 2 0 0 2 0 0 2 2 2 0 2 0 2 2 0 0 2 2 2 2 2 1 2 0 0 2 1 0 0 0 0 2 0 2 0 0 0 0\"',\n",
              " 'InformationStateString(1) = \"0 2 2 2 0 2 0 0 2 2 0 0 2 0 0 2 0 0 2 2 2 0 2 0 2 2 0 0 2 2 2 2 2 1 2 0 0 2 1 0 0 0 0 2 0 2 0 0 0 0\"',\n",
              " 'InformationStateTensor(0).observation: zeros(2500)',\n",
              " 'InformationStateTensor(1).observation: zeros(2500)',\n",
              " 'ObservationString(0) = \"0 2 2 2 0 2 0 0 2 2 0 0 2 0 0 2 0 0 2 2 2 0 2 0 2 2 0 0 2 2 2 2 2 1 2 0 0 2 1 0 0 0 0 2 0 2 0 0 0 0\"',\n",
              " 'ObservationString(1) = \"0 2 2 2 0 2 0 0 2 2 0 0 2 0 0 2 0 0 2 2 2 0 2 0 2 2 0 0 2 2 2 2 2 1 2 0 0 2 1 0 0 0 0 2 0 2 0 0 0 0\"',\n",
              " 'PublicObservationString() = \"0 2 2 2 0 2 0 0 2 2 0 0 2 0 0 2 0 0 2 2 2 0 2 0 2 2 0 0 2 2 2 2 2 1 2 0 0 2 1 0 0 0 0 2 0 2 0 0 0 0\"',\n",
              " 'PrivateObservationString(0) = \"0 2 2 2 0 2 0 0 2 2 0 0 2 0 0 2 0 0 2 2 2 0 2 0 2 2 0 0 2 2 2 2 2 1 2 0 0 2 1 0 0 0 0 2 0 2 0 0 0 0\"',\n",
              " 'PrivateObservationString(1) = \"0 2 2 2 0 2 0 0 2 2 0 0 2 0 0 2 0 0 2 2 2 0 2 0 2 2 0 0 2 2 2 2 2 1 2 0 0 2 1 0 0 0 0 2 0 2 0 0 0 0\"',\n",
              " 'ObservationTensor(0): zeros(2500)',\n",
              " 'ObservationTensor(1): zeros(2500)',\n",
              " 'Rewards() = [ 0. -0.]',\n",
              " 'Returns() = [0. 0.]']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from open_spiel.python.algorithms import generate_playthrough\n",
        "\n",
        "generate_playthrough.playthrough_lines(\"graph_attack_defend\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cImmCfePicXr"
      },
      "source": [
        "## Run Random Game"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "v-h1reLLbIsw",
        "outputId": "207ab0cc-6c6d-41cc-b027-9b07a72a6822"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pyspiel\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "\n",
        "def drawNetwork(graph,name):\n",
        "    d = nx.get_node_attributes(graph, \"active\") \n",
        "    low, *_, high = sorted(d.values())\n",
        "    norm = mpl.colors.Normalize(vmin=low, vmax=high, clip=True)\n",
        "    mapper = mpl.cm.ScalarMappable(norm=norm, cmap=mpl.cm.coolwarm_r)\n",
        "    nx.draw_kamada_kawai(graph, \n",
        "            nodelist=d,\n",
        "            labels=d,\n",
        "            node_color=[mapper.to_rgba(i) \n",
        "                        for i in d.values()], \n",
        "            with_labels=True,\n",
        "            font_color='white')\n",
        "    plt.title(\"All Nodes\")\n",
        "    plt.show()\n",
        "    #plt.savefig(\"./0/\"+name)\n",
        "    plt.clf()\n",
        "\n",
        "def get_degree(subgraph):\n",
        "    degree_node = np.array(subgraph.degree())\n",
        "    sorted = degree_node[np.argsort(degree_node[:, 1])]\n",
        "    sorted = sorted[::-1]\n",
        "    return sorted\n",
        "pyspiel.register_game(_GAME_TYPE, GraphGame)\n",
        "game = pyspiel.load_game(\"graph_attack_defend\")\n",
        "state = game.new_initial_state()\n",
        "i = 0\n",
        "while not state.is_terminal():\n",
        "    legal_actions = state.legal_actions()\n",
        "    acc = state.board.subgraph(legal_actions)\n",
        "    sorted_nodes = get_degree(acc)\n",
        "    act = sorted_nodes[:,0]\n",
        "    # The algorithm can pick an action based on an observation (fully observable\n",
        "    # games) or an information state (information available for that player)\n",
        "    # We arbitrarily select the first available action as an example.\n",
        "    #action = act[0:2]\n",
        "    action = np.random.choice(legal_actions,2)\n",
        "    state.apply_actions(action)\n",
        "    i+=1\n",
        "plt.plot(state.lcc,color='red')\n",
        "plt.plot(state.r,color='green')\n",
        "plt.title(\"LCC/n vs iteration\")\n",
        "#plt.savefig(\"LCC_Plot_Random\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlCHRwspvaYK"
      },
      "source": [
        "# RL Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3LA75Y_O0qBY"
      },
      "outputs": [],
      "source": [
        "import torch "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sudsr6BGduuq"
      },
      "source": [
        "## RL Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GCaO4gSLdwat"
      },
      "outputs": [],
      "source": [
        "# Copyright 2019 DeepMind Technologies Ltd. All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Reinforcement Learning (RL) Environment for Open Spiel.\n",
        "\n",
        "This module wraps Open Spiel Python interface providing an RL-friendly API. It\n",
        "covers both turn-based and simultaneous move games. Interactions between agents\n",
        "and the underlying game occur mostly through the `reset` and `step` methods,\n",
        "which return a `TimeStep` structure (see its docstrings for more info).\n",
        "\n",
        "The following example illustrates the interaction dynamics. Consider a 2-player\n",
        "Kuhn Poker (turn-based game). Agents have access to the `observations` (a dict)\n",
        "field from `TimeSpec`, containing the following members:\n",
        " * `info_state`: list containing the game information state for each player. The\n",
        "   size of the list always correspond to the number of players. E.g.:\n",
        "   [[0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0]].\n",
        " * `legal_actions`: list containing legal action ID lists (one for each player).\n",
        "   E.g.: [[0, 1], [0]], which corresponds to actions 0 and 1 being valid for\n",
        "   player 0 (the 1st player) and action 0 being valid for player 1 (2nd player).\n",
        " * `current_player`: zero-based integer representing the player to make a move.\n",
        "\n",
        "At each `step` call, the environment expects a singleton list with the action\n",
        "(as it's a turn-based game), e.g.: [1]. This (zero-based) action must correspond\n",
        "to the player specified at `current_player`. The game (which is at decision\n",
        "node) will process the action and take as many steps necessary to cover chance\n",
        "nodes, halting at a new decision or final node. Finally, a new `TimeStep`is\n",
        "returned to the agent.\n",
        "\n",
        "Simultaneous-move games follow analogous dynamics. The only differences is the\n",
        "environment expects a list of actions, one per player. Note the `current_player`\n",
        "field is \"irrelevant\" here, admitting a constant value defined in spiel.h, which\n",
        "defaults to -2 (module level constant `SIMULTANEOUS_PLAYER_ID`).\n",
        "\n",
        "See open_spiel/python/examples/rl_example.py for example usages.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "\n",
        "import enum\n",
        "from absl import logging\n",
        "import numpy as np\n",
        "\n",
        "import pyspiel\n",
        "\n",
        "SIMULTANEOUS_PLAYER_ID = pyspiel.PlayerId.SIMULTANEOUS\n",
        "\n",
        "class TimeStep(\n",
        "    collections.namedtuple(\n",
        "        \"TimeStep\", [\"observations\", \"rewards\", \"discounts\", \"step_type\"])):\n",
        "  \"\"\"Returned with every call to `step` and `reset`.\n",
        "\n",
        "  A `TimeStep` contains the data emitted by a game at each step of interaction.\n",
        "  A `TimeStep` holds an `observation` (list of dicts, one per player),\n",
        "  associated lists of `rewards`, `discounts` and a `step_type`.\n",
        "\n",
        "  The first `TimeStep` in a sequence will have `StepType.FIRST`. The final\n",
        "  `TimeStep` will have `StepType.LAST`. All other `TimeStep`s in a sequence will\n",
        "  have `StepType.MID.\n",
        "\n",
        "  Attributes:\n",
        "    observations: a list of dicts containing observations per player.\n",
        "    rewards: A list of scalars (one per player), or `None` if `step_type` is\n",
        "      `StepType.FIRST`, i.e. at the start of a sequence.\n",
        "    discounts: A list of discount values in the range `[0, 1]` (one per player),\n",
        "      or `None` if `step_type` is `StepType.FIRST`.\n",
        "    step_type: A `StepType` enum value.\n",
        "  \"\"\"\n",
        "  __slots__ = ()\n",
        "\n",
        "  def first(self):\n",
        "    return self.step_type == StepType.FIRST\n",
        "\n",
        "  def mid(self):\n",
        "    return self.step_type == StepType.MID\n",
        "\n",
        "  def last(self):\n",
        "    return self.step_type == StepType.LAST\n",
        "\n",
        "  def is_simultaneous_move(self):\n",
        "    return self.observations[\"current_player\"] == SIMULTANEOUS_PLAYER_ID\n",
        "\n",
        "  def current_player(self):\n",
        "    return self.observations[\"current_player\"]\n",
        "\n",
        "\n",
        "class StepType(enum.Enum):\n",
        "  \"\"\"Defines the status of a `TimeStep` within a sequence.\"\"\"\n",
        "\n",
        "  FIRST = 0  # Denotes the first `TimeStep` in a sequence.\n",
        "  MID = 1  # Denotes any `TimeStep` in a sequence that is not FIRST or LAST.\n",
        "  LAST = 2  # Denotes the last `TimeStep` in a sequence.\n",
        "\n",
        "  def first(self):\n",
        "    return self is StepType.FIRST\n",
        "\n",
        "  def mid(self):\n",
        "    return self is StepType.MID\n",
        "\n",
        "  def last(self):\n",
        "    return self is StepType.LAST\n",
        "\n",
        "\n",
        "# Global pyspiel members\n",
        "def registered_games():\n",
        "  return pyspiel.registered_games()\n",
        "\n",
        "\n",
        "\n",
        "class ChanceEventSampler(object):\n",
        "  \"\"\"Default sampler for external chance events.\"\"\"\n",
        "\n",
        "  def __init__(self, seed=None):\n",
        "    self.seed(seed)\n",
        "\n",
        "  def seed(self, seed=None):\n",
        "    self._rng = np.random.RandomState(seed)\n",
        "\n",
        "  def __call__(self, state):\n",
        "    \"\"\"Sample a chance event in the given state.\"\"\"\n",
        "    actions, probs = zip(*state.chance_outcomes())\n",
        "    return self._rng.choice(actions, p=probs)\n",
        "\n",
        "\n",
        "class ObservationType(enum.Enum):\n",
        "  \"\"\"Defines what kind of observation to use.\"\"\"\n",
        "  OBSERVATION = 0  # Use observation_tensor\n",
        "  INFORMATION_STATE = 1  # Use information_state_tensor\n",
        "\n",
        "\n",
        "class Environment(object):\n",
        "  \"\"\"Open Spiel reinforcement learning environment class.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               game,\n",
        "               discount=1.0,\n",
        "               chance_event_sampler=None,\n",
        "               observation_type=None,\n",
        "               include_full_state=False,\n",
        "               distribution=None,\n",
        "               mfg_population=None,\n",
        "               enable_legality_check=False,\n",
        "               **kwargs):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      game: [string, pyspiel.Game] Open Spiel game name or game instance.\n",
        "      discount: float, discount used in non-initial steps. Defaults to 1.0.\n",
        "      chance_event_sampler: optional object with `sample_external_events` method\n",
        "        to sample chance events.\n",
        "      observation_type: what kind of observation to use. If not specified, will\n",
        "        default to INFORMATION_STATE unless the game doesn't provide it.\n",
        "      include_full_state: whether or not to include the full serialized\n",
        "        OpenSpiel state in the observations (sometimes useful for debugging).\n",
        "      distribution: the distribution over states if the game is a mean field\n",
        "        game.\n",
        "      mfg_population: The Mean Field Game population to consider.\n",
        "      enable_legality_check: Check the legality of the move before stepping.\n",
        "      **kwargs: dict, additional settings passed to the Open Spiel game.\n",
        "    \"\"\"\n",
        "    self._chance_event_sampler = chance_event_sampler or ChanceEventSampler()\n",
        "    self._include_full_state = include_full_state\n",
        "    self._distribution = distribution\n",
        "    self._mfg_population = mfg_population\n",
        "    self._enable_legality_check = enable_legality_check\n",
        "\n",
        "    if isinstance(game, str):\n",
        "      if kwargs:\n",
        "        game_settings = {key: val for (key, val) in kwargs.items()}\n",
        "        logging.info(\"Using game settings: %s\", game_settings)\n",
        "        self._game = pyspiel.load_game(game, game_settings)\n",
        "      else:\n",
        "        logging.info(\"Using game string: %s\", game)\n",
        "        self._game = pyspiel.load_game(game)\n",
        "    else:  # pyspiel.Game or API-compatible object.\n",
        "      logging.info(\"Using game instance: %s\", game.get_type().short_name)\n",
        "      self._game = game\n",
        "\n",
        "    self._num_players = self._game.num_players()\n",
        "    self._state = None\n",
        "    self._should_reset = True\n",
        "\n",
        "    # Discount returned at non-initial  steps.\n",
        "    self._discounts = [discount] * self._num_players\n",
        "\n",
        "    # Determine what observation type to use.\n",
        "    if observation_type is None:\n",
        "      if self._game.get_type().provides_information_state_tensor:\n",
        "        observation_type = ObservationType.INFORMATION_STATE\n",
        "      else:\n",
        "        observation_type = ObservationType.OBSERVATION\n",
        "\n",
        "    # Check the requested observation type is supported.\n",
        "    if observation_type == ObservationType.OBSERVATION:\n",
        "      if not self._game.get_type().provides_observation_tensor:\n",
        "        raise ValueError(f\"observation_tensor not supported by {game}\")\n",
        "    elif observation_type == ObservationType.INFORMATION_STATE:\n",
        "      if not self._game.get_type().provides_information_state_tensor:\n",
        "        raise ValueError(f\"information_state_tensor not supported by {game}\")\n",
        "    self._use_observation = (observation_type == ObservationType.OBSERVATION)\n",
        "\n",
        "    if self._game.get_type().dynamics == pyspiel.GameType.Dynamics.MEAN_FIELD:\n",
        "      assert distribution is not None\n",
        "      assert mfg_population is not None\n",
        "      assert 0 <= mfg_population < self._num_players\n",
        "\n",
        "  def seed(self, seed=None):\n",
        "    self._chance_event_sampler.seed(seed)\n",
        "\n",
        "  def get_time_step(self):\n",
        "    \"\"\"Returns a `TimeStep` without updating the environment.\n",
        "\n",
        "    Returns:\n",
        "      A `TimeStep` namedtuple containing:\n",
        "        observation: list of dicts containing one observations per player, each\n",
        "          corresponding to `observation_spec()`.\n",
        "        reward: list of rewards at this timestep, or None if step_type is\n",
        "          `StepType.FIRST`.\n",
        "        discount: list of discounts in the range [0, 1], or None if step_type is\n",
        "          `StepType.FIRST`.\n",
        "        step_type: A `StepType` value.\n",
        "    \"\"\"\n",
        "    observations = {\n",
        "        \"num_nodes\":[],\n",
        "        \"info_state\": [],\n",
        "        \"x\":[],\n",
        "        \"legal_actions\": [],\n",
        "        \"current_player\": [],\n",
        "        \"serialized_state\": []\n",
        "    }\n",
        "    rewards = []\n",
        "    step_type = StepType.LAST if self._state.is_terminal() else StepType.MID\n",
        "    self._should_reset = step_type == StepType.LAST\n",
        "    cur_rewards = self._state.rewards()\n",
        "    for player_id in range(self.num_players):\n",
        "      rewards.append(cur_rewards[player_id])\n",
        "      observations[\"info_state\"].append(\n",
        "          self._state.observation_tensor(player_id) if self._use_observation\n",
        "          else self._state.information_state_tensor(player_id))\n",
        "      observations[\"legal_actions\"].append(self._state.legal_actions(player_id))\n",
        "    observations[\"num_nodes\"] = self._state.num_nodes\n",
        "    observations[\"x\"]=self._state.x\n",
        "    observations[\"current_player\"] = self._state.current_player()\n",
        "    discounts = self._discounts\n",
        "    if step_type == StepType.LAST:\n",
        "      # When the game is in a terminal state set the discount to 0.\n",
        "      discounts = [0. for _ in discounts]\n",
        "\n",
        "    if self._include_full_state:\n",
        "      observations[\"serialized_state\"] = pyspiel.serialize_game_and_state(\n",
        "          self._game, self._state)\n",
        "\n",
        "    return TimeStep(\n",
        "        observations=observations,\n",
        "        rewards=rewards,\n",
        "        discounts=discounts,\n",
        "        step_type=step_type)\n",
        "\n",
        "  def _check_legality(self, actions):\n",
        "    if self.is_turn_based:\n",
        "      legal_actions = self._state.legal_actions()\n",
        "      if actions[0] not in legal_actions:\n",
        "        raise RuntimeError(f\"step() called on illegal action {actions[0]}\")\n",
        "    else:\n",
        "      for p in range(len(actions)):\n",
        "        legal_actions = self._state.legal_actions(p)\n",
        "        if legal_actions and actions[p] not in legal_actions:\n",
        "          raise RuntimeError(f\"step() by player {p} called on illegal \" +\n",
        "                             f\"action: {actions[p]}\")\n",
        "\n",
        "  def step(self, actions):\n",
        "    \"\"\"Updates the environment according to `actions` and returns a `TimeStep`.\n",
        "\n",
        "    If the environment returned a `TimeStep` with `StepType.LAST` at the\n",
        "    previous step, this call to `step` will start a new sequence and `actions`\n",
        "    will be ignored.\n",
        "\n",
        "    This method will also start a new sequence if called after the environment\n",
        "    has been constructed and `reset` has not been called. Again, in this case\n",
        "    `actions` will be ignored.\n",
        "\n",
        "    Args:\n",
        "      actions: a list containing one action per player, following specifications\n",
        "        defined in `action_spec()`.\n",
        "\n",
        "    Returns:\n",
        "      A `TimeStep` namedtuple containing:\n",
        "        observation: list of dicts containing one observations per player, each\n",
        "          corresponding to `observation_spec()`.\n",
        "        reward: list of rewards at this timestep, or None if step_type is\n",
        "          `StepType.FIRST`.\n",
        "        discount: list of discounts in the range [0, 1], or None if step_type is\n",
        "          `StepType.FIRST`.\n",
        "        step_type: A `StepType` value.\n",
        "    \"\"\"\n",
        "    assert len(actions) == self.num_actions_per_step, (\n",
        "        \"Invalid number of actions! Expected {}\".format(\n",
        "            self.num_actions_per_step))\n",
        "    if self._should_reset:\n",
        "      return self.reset()\n",
        "\n",
        "    if self._enable_legality_check:\n",
        "      self._check_legality(actions)\n",
        "\n",
        "    if self.is_turn_based:\n",
        "      self._state.apply_action(actions[0])\n",
        "    else:\n",
        "      self._state.apply_actions(actions)\n",
        "    self._sample_external_events()\n",
        "\n",
        "    return self.get_time_step()\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Starts a new sequence and returns the first `TimeStep` of this sequence.\n",
        "\n",
        "    Returns:\n",
        "      A `TimeStep` namedtuple containing:\n",
        "        observations: list of dicts containing one observations per player, each\n",
        "          corresponding to `observation_spec()`.\n",
        "        rewards: list of rewards at this timestep, or None if step_type is\n",
        "          `StepType.FIRST`.\n",
        "        discounts: list of discounts in the range [0, 1], or None if step_type\n",
        "          is `StepType.FIRST`.\n",
        "        step_type: A `StepType` value.\n",
        "    \"\"\"\n",
        "    self._should_reset = False\n",
        "    if self._game.get_type(\n",
        "    ).dynamics == pyspiel.GameType.Dynamics.MEAN_FIELD and self._num_players > 1:\n",
        "      self._state = self._game.new_initial_state_for_population(\n",
        "          self._mfg_population)\n",
        "    else:\n",
        "      self._state = self._game.new_initial_state()\n",
        "    self._sample_external_events()\n",
        "\n",
        "    observations = {\n",
        "        \"num_nodes\":[],\n",
        "        \"info_state\": [],\n",
        "        \"x\":[],\n",
        "        \"legal_actions\": [],\n",
        "        \"current_player\": [],\n",
        "        \"serialized_state\": []\n",
        "    }\n",
        "    for player_id in range(self.num_players):\n",
        "      observations[\"info_state\"].append(\n",
        "          self._state.observation_tensor(player_id) if self._use_observation\n",
        "          else self._state.information_state_tensor(player_id))\n",
        "      observations[\"legal_actions\"].append(self._state.legal_actions(player_id))\n",
        "    observations[\"x\"]=self._state.x\n",
        "    observations[\"num_nodes\"]= self._state.num_nodes\n",
        "    observations[\"current_player\"] = self._state.current_player()\n",
        "    if self._include_full_state:\n",
        "      observations[\"serialized_state\"] = pyspiel.serialize_game_and_state(\n",
        "          self._game, self._state)\n",
        "\n",
        "    return TimeStep(\n",
        "        observations=observations,\n",
        "        rewards=None,\n",
        "        discounts=None,\n",
        "        step_type=StepType.FIRST)\n",
        "\n",
        "  def _sample_external_events(self):\n",
        "    \"\"\"Sample chance events until we get to a decision node.\"\"\"\n",
        "    while self._state.is_chance_node() or (self._state.current_player()\n",
        "                                           == pyspiel.PlayerId.MEAN_FIELD):\n",
        "      if self._state.is_chance_node():\n",
        "        outcome = self._chance_event_sampler(self._state)\n",
        "        self._state.apply_action(outcome)\n",
        "      if self._state.current_player() == pyspiel.PlayerId.MEAN_FIELD:\n",
        "        dist_to_register = self._state.distribution_support()\n",
        "        dist = [\n",
        "            self._distribution.value_str(str_state, default_value=0.0)\n",
        "            for str_state in dist_to_register\n",
        "        ]\n",
        "        self._state.update_distribution(dist)\n",
        "\n",
        "  def observation_spec(self):\n",
        "    \"\"\"Defines the observation per player provided by the environment.\n",
        "\n",
        "    Each dict member will contain its expected structure and shape. E.g.: for\n",
        "    Kuhn Poker {\"info_state\": (6,), \"legal_actions\": (2,), \"current_player\": (),\n",
        "                \"serialized_state\": ()}\n",
        "\n",
        "    Returns:\n",
        "      A specification dict describing the observation fields and shapes.\n",
        "    \"\"\"\n",
        "    return dict(\n",
        "        info_state=tuple([\n",
        "            self._game.observation_tensor if self._use_observation else\n",
        "            self._game.information_state_tensor_size\n",
        "        ]),\n",
        "        num_nodes=(),\n",
        "        x=(),\n",
        "        legal_actions=(self._game.num_distinct_actions(),),\n",
        "        current_player=(),\n",
        "        serialized_state=(),\n",
        "    )\n",
        "\n",
        "  def action_spec(self):\n",
        "    \"\"\"Defines per player action specifications.\n",
        "\n",
        "    Specifications include action boundaries and their data type.\n",
        "    E.g.: for Kuhn Poker {\"num_actions\": 2, \"min\": 0, \"max\":1, \"dtype\": int}\n",
        "\n",
        "    Returns:\n",
        "      A specification dict containing per player action properties.\n",
        "    \"\"\"\n",
        "    return dict(\n",
        "        num_actions=self._game.num_distinct_actions(),\n",
        "        min=0,\n",
        "        max=self._game.num_distinct_actions() - 1,\n",
        "        dtype=int,\n",
        "    )\n",
        "\n",
        "  # Environment properties\n",
        "  @property\n",
        "  def use_observation(self):\n",
        "    \"\"\"Returns whether the environment is using the game's observation.\n",
        "\n",
        "    If false, it is using the game's information state.\n",
        "    \"\"\"\n",
        "    return self._use_observation\n",
        "\n",
        "  # Game properties\n",
        "  @property\n",
        "  def name(self):\n",
        "    return self._game.get_type().short_name\n",
        "\n",
        "  @property\n",
        "  def num_players(self):\n",
        "    return self._game.num_players()\n",
        "\n",
        "  @property\n",
        "  def num_actions_per_step(self):\n",
        "    return 1 if self.is_turn_based else self.num_players\n",
        "\n",
        "  # New RL calls for more advanced use cases (e.g. search + RL).\n",
        "  @property\n",
        "  def is_turn_based(self):\n",
        "    return ((self._game.get_type().dynamics\n",
        "             == pyspiel.GameType.Dynamics.SEQUENTIAL) or\n",
        "            (self._game.get_type().dynamics\n",
        "             == pyspiel.GameType.Dynamics.MEAN_FIELD))\n",
        "\n",
        "  @property\n",
        "  def max_game_length(self):\n",
        "    return self._game.max_game_length()\n",
        "\n",
        "  @property\n",
        "  def is_chance_node(self):\n",
        "    return self._state.is_chance_node()\n",
        "\n",
        "  @property\n",
        "  def game(self):\n",
        "    return self._game\n",
        "\n",
        "  def set_state(self, new_state):\n",
        "    \"\"\"Updates the game state.\"\"\"\n",
        "    assert new_state.get_game() == self.game, (\n",
        "        \"State must have been created by the same game.\")\n",
        "    self._state = new_state\n",
        "\n",
        "  @property\n",
        "  def get_state(self):\n",
        "    return self._state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSYvGMVa1m9q"
      },
      "source": [
        "## Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GdZD4I7f1mpW"
      },
      "outputs": [],
      "source": [
        "# Copyright 2019 DeepMind Technologies Limited\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"Replay buffer of fixed size with a FIFI replacement policy.\"\"\"\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "  \"\"\"ReplayBuffer of fixed size with a FIFO replacement policy.\n",
        "  Stored transitions can be sampled uniformly.\n",
        "  The underlying datastructure is a ring buffer, allowing 0(1) adding and\n",
        "  sampling.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, replay_buffer_capacity):\n",
        "    self._replay_buffer_capacity = replay_buffer_capacity\n",
        "    self._data = []\n",
        "    self._next_entry_index = 0\n",
        "\n",
        "  def add(self, element):\n",
        "    \"\"\"Adds `element` to the buffer.\n",
        "    If the buffer is full, the oldest element will be replaced.\n",
        "    Args:\n",
        "      element: data to be added to the buffer.\n",
        "    \"\"\"\n",
        "    if len(self._data) < self._replay_buffer_capacity:\n",
        "      self._data.append(element)\n",
        "    else:\n",
        "      self._data[self._next_entry_index] = element\n",
        "      self._next_entry_index += 1\n",
        "      self._next_entry_index %= self._replay_buffer_capacity\n",
        "\n",
        "  def sample(self, num_samples):\n",
        "    \"\"\"Returns `num_samples` uniformly sampled from the buffer.\n",
        "    Args:\n",
        "      num_samples: `int`, number of samples to draw.\n",
        "    Returns:\n",
        "      An iterable over `num_samples` random elements of the buffer.\n",
        "    Raises:\n",
        "      ValueError: If there are less than `num_samples` elements in the buffer\n",
        "    \"\"\"\n",
        "    if len(self._data) < num_samples:\n",
        "      raise ValueError(\"{} elements could not be sampled from size {}\".format(\n",
        "          num_samples, len(self._data)))\n",
        "    return random.sample(self._data, num_samples)\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Resets the contents of the replay buffer.\"\"\"\n",
        "    self._data = []\n",
        "    self._next_entry_index = 0\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self._data)\n",
        "\n",
        "  def __iter__(self):\n",
        "    return iter(self._data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32SGt_aG09dx"
      },
      "source": [
        "## DQN PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "N-QQj8Y6088b"
      },
      "outputs": [],
      "source": [
        "# Copyright 2019 DeepMind Technologies Limited\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"DQN agent implemented in PyTorch.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy import sparse\n",
        "from torch._C import dtype\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch_geometric.nn import GATv2Conv\n",
        "from torch.nn import Flatten, Linear\n",
        "from torch_geometric.utils import convert\n",
        "\n",
        "from open_spiel.python import rl_agent\n",
        "#from open_spiel.python.utils.replay_buffer import ReplayBuffer\n",
        "\n",
        "Transition = collections.namedtuple(\n",
        "    \"Transition\",\n",
        "    \"info_state x action reward next_info_state next_x is_final_step legal_actions_mask\")\n",
        "\n",
        "ILLEGAL_ACTION_LOGITS_PENALTY = -1e9\n",
        "\n",
        "\n",
        "class GraphNN(nn.Module):\n",
        "  \"\"\"A simple network built from nn.linear layers.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               feature_size,\n",
        "               hidden_sizes):\n",
        "    \"\"\"Create the MLP.\n",
        "    Args:\n",
        "      input_size: (int) number of inputs\n",
        "      hidden_sizes: (list) sizes (number of units) of each hidden layer\n",
        "      output_size: (int) number of outputs\n",
        "      activate_final: (bool) should final layer should include a ReLU\n",
        "    \"\"\"\n",
        "\n",
        "    super(GraphNN, self).__init__()\n",
        "    '''self.conv1 = GATv2Conv(feature_size, hidden_sizes,add_self_loops = True,heads=2) \n",
        "    self.conv2 = GATv2Conv(hidden_sizes*2,hidden_sizes,add_self_loops = True,heads=2) \n",
        "    self.linear = nn.Linear(hidden_sizes* 2, 1)'''\n",
        "    self.conv1 = GATv2Conv(feature_size, hidden_sizes) \n",
        "    self.conv2 = GATv2Conv(hidden_sizes,hidden_sizes) \n",
        "    self.linear = nn.Linear(hidden_sizes, 1)\n",
        "  \n",
        "  def forward(self, node_feature, edge_index):\n",
        "    x, edge_index = node_feature, edge_index\n",
        "    x = F.relu(self.conv1(x, edge_index))\n",
        "    #x = F.dropout(x, training=self.training)\n",
        "    x = F.relu(self.conv2(x, edge_index))\n",
        "    x = self.linear(x)\n",
        "    x = torch.softmax(x,dim=0)\n",
        "    return x\n",
        "\n",
        "\n",
        "class DQN(rl_agent.AbstractAgent):\n",
        "  \"\"\"DQN Agent implementation in PyTorch.\n",
        "  See open_spiel/python/examples/breakthrough_dqn.py for an usage example.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               player_id,\n",
        "               state_representation_size,\n",
        "               num_actions,\n",
        "               hidden_layers_sizes=5,\n",
        "               output_layer_size =1,\n",
        "               replay_buffer_capacity=10000,\n",
        "               batch_size=128,\n",
        "               replay_buffer_class=ReplayBuffer,\n",
        "               learning_rate=0.001,\n",
        "               update_target_network_every=1000,\n",
        "               learn_every=10,\n",
        "               discount_factor=1.0,\n",
        "               min_buffer_size_to_learn=1000,\n",
        "               epsilon_start=1.0,\n",
        "               epsilon_end=0.1,\n",
        "               epsilon_decay_duration=int(1e6),\n",
        "               optimizer_str=\"adam\",\n",
        "               loss_str=\"huber\"):\n",
        "    \"\"\"Initialize the DQN agent.\"\"\"\n",
        "\n",
        "    # This call to locals() is used to store every argument used to initialize\n",
        "    # the class instance, so it can be copied with no hyperparameter change.\n",
        "    self._kwargs = locals()\n",
        "\n",
        "    self.player_id = player_id\n",
        "    self._num_actions = num_actions\n",
        "    \"\"\"if isinstance(hidden_layers_sizes, int):\n",
        "      hidden_layers_sizes = [hidden_layers_sizes]\"\"\"\n",
        "    self.num_feature = state_representation_size\n",
        "    self._layer_sizes = hidden_layers_sizes\n",
        "    self._batch_size = batch_size\n",
        "    self._update_target_network_every = update_target_network_every\n",
        "    self._learn_every = learn_every\n",
        "    self._min_buffer_size_to_learn = min_buffer_size_to_learn\n",
        "    self._discount_factor = discount_factor\n",
        "\n",
        "    self._epsilon_start = epsilon_start\n",
        "    self._epsilon_end = epsilon_end\n",
        "    self._epsilon_decay_duration = epsilon_decay_duration\n",
        "\n",
        "    # TODO(author6) Allow for optional replay buffer config.\n",
        "    if not isinstance(replay_buffer_capacity, int):\n",
        "      raise ValueError(\"Replay buffer capacity not an integer.\")\n",
        "    self._replay_buffer = replay_buffer_class(replay_buffer_capacity)\n",
        "    self._prev_timestep = None\n",
        "    self._prev_action = None\n",
        "\n",
        "    # Step counter to keep track of learning, eps decay and target network.\n",
        "    self._step_counter = 0\n",
        "\n",
        "    # Keep track of the last training loss achieved in an update step.\n",
        "    self._last_loss_value = None\n",
        "\n",
        "    # Create the Q-network instances\n",
        "    self._q_network = GraphNN(state_representation_size, self._layer_sizes) #num_actions\n",
        "\n",
        "    self._target_q_network = GraphNN(state_representation_size, self._layer_sizes)\n",
        "    # Q network outputs approx single feature embedded value = approx q value for each Noder\n",
        "    if loss_str == \"mse\":\n",
        "      self.loss_class = F.mse_loss\n",
        "    elif loss_str == \"huber\":\n",
        "      self.loss_class = F.smooth_l1_loss\n",
        "    else:\n",
        "      raise ValueError(\"Not implemented, choose from 'mse', 'huber'.\")\n",
        "\n",
        "    if optimizer_str == \"adam\":\n",
        "      self._optimizer = torch.optim.Adam(\n",
        "          self._q_network.parameters(), lr=learning_rate)\n",
        "    elif optimizer_str == \"sgd\":\n",
        "      self._optimizer = torch.optim.SGD(\n",
        "          self._q_network.parameters(), lr=learning_rate)\n",
        "    else:\n",
        "      raise ValueError(\"Not implemented, choose from 'adam' and 'sgd'.\")\n",
        "\n",
        "  def step(self, time_step, is_evaluation=False, add_transition_record=True):\n",
        "    \"\"\"Returns the action to be taken and updates the Q-network if needed.\n",
        "    Args:\n",
        "      time_step: an instance of rl_environment.TimeStep.\n",
        "      is_evaluation: bool, whether this is a training or evaluation call.\n",
        "      add_transition_record: Whether to add to the replay buffer on this step.\n",
        "    Returns:\n",
        "      A `rl_agent.StepOutput` containing the action probs and chosen action.\n",
        "    \"\"\"\n",
        "\n",
        "    # Act step: don't act at terminal info states or if its not our turn.\n",
        "    if (not time_step.last()) and (\n",
        "        time_step.is_simultaneous_move() or\n",
        "        self.player_id == time_step.current_player()):\n",
        "      num_nodes = time_step.observations[\"num_nodes\"]\n",
        "      info_state = time_step.observations[\"info_state\"][self.player_id]\n",
        "      legal_actions = time_step.observations[\"legal_actions\"][self.player_id]\n",
        "      x = time_step.observations[\"x\"]\n",
        "      epsilon = self._get_epsilon(is_evaluation)\n",
        "      action, probs = self._epsilon_greedy(num_nodes,info_state,x, legal_actions, epsilon)\n",
        "    else:\n",
        "      action = None\n",
        "      probs = []\n",
        "    #if legal_actions <= 5\n",
        "    # Don't mess up with the state during evaluation.\n",
        "    if not is_evaluation:\n",
        "      self._step_counter += 1\n",
        "\n",
        "      if self._step_counter % self._learn_every == 0:\n",
        "        self._last_loss_value = self.learn()\n",
        "\n",
        "      if self._step_counter % self._update_target_network_every == 0:\n",
        "        # state_dict method returns a dictionary containing a whole state of the\n",
        "        # module.\n",
        "        self._target_q_network.load_state_dict(self._q_network.state_dict())\n",
        "\n",
        "      if self._prev_timestep and add_transition_record:\n",
        "        # We may omit record adding here if it's done elsewhere.\n",
        "        self.add_transition(self._prev_timestep, self._prev_action, time_step)\n",
        "\n",
        "      if time_step.last():  # prepare for the next episode.\n",
        "        self._prev_timestep = None\n",
        "        self._prev_action = None\n",
        "        return\n",
        "      else:\n",
        "        self._prev_timestep = time_step\n",
        "        self._prev_action = action\n",
        "\n",
        "    return rl_agent.StepOutput(action=action, probs=probs)\n",
        "\n",
        "  def add_transition(self, prev_time_step, prev_action, time_step):\n",
        "    \"\"\"Adds the new transition using `time_step` to the replay buffer.\n",
        "    Adds the transition from `self._prev_timestep` to `time_step` by\n",
        "    `self._prev_action`.\n",
        "    Args:\n",
        "      prev_time_step: prev ts, an instance of rl_environment.TimeStep.\n",
        "      prev_action: int, action taken at `prev_time_step`.\n",
        "      time_step: current ts, an instance of rl_environment.TimeStep.\n",
        "    \"\"\"\n",
        "    assert prev_time_step is not None\n",
        "    _num_actions = time_step.observations[\"num_nodes\"]\n",
        "    legal_actions = (time_step.observations[\"legal_actions\"][self.player_id])\n",
        "    legal_actions_mask = np.zeros(_num_actions)\n",
        "    legal_actions_mask[legal_actions] = 1.0\n",
        "    transition = Transition(\n",
        "        info_state=(\n",
        "            prev_time_step.observations[\"info_state\"][self.player_id][:]),\n",
        "        x = prev_time_step.observations[\"x\"],\n",
        "        action=prev_action,\n",
        "        reward=time_step.rewards[self.player_id],\n",
        "        next_info_state=time_step.observations[\"info_state\"][self.player_id][:],\n",
        "        next_x = time_step.observations[\"x\"],\n",
        "        is_final_step=float(time_step.last()),\n",
        "        legal_actions_mask=legal_actions_mask)\n",
        "    self._replay_buffer.add(transition)\n",
        "\n",
        "  def _epsilon_greedy(self,num_nodes, info_state,x, legal_actions, epsilon):\n",
        "    \"\"\"Returns a valid epsilon-greedy action and valid action probs.\n",
        "    Action probabilities are given by a softmax over legal q-values.\n",
        "    Args:\n",
        "      info_state: hashable representation of the information state.\n",
        "      legal_actions: list of legal actions at `info_state`.\n",
        "      epsilon: float, probability of taking an exploratory action.\n",
        "    Returns:\n",
        "      A valid epsilon-greedy action and valid action probabilities.\n",
        "    \"\"\"\n",
        "    size = int(num_nodes)\n",
        "    probs = np.zeros(size)\n",
        "    if np.random.rand() < epsilon:\n",
        "      action = np.random.choice(legal_actions)\n",
        "      probs[legal_actions] = 1.0 / len(legal_actions)\n",
        "    else:\n",
        "      #info_state = torch.Tensor(np.reshape(info_state, [1, -1]))\n",
        "      #get values in terms of (Data infostate, Edgelist)\n",
        "      shape = (size,size)\n",
        "      edges = np.reshape(info_state, shape)\n",
        "      sA = sparse.coo_matrix(edges) \n",
        "      edge_index, _  = convert.from_scipy_sparse_matrix(sA)\n",
        "      data = torch.from_numpy(x.astype(np.float32))\n",
        "      q_values = self._q_network(data.type(torch.FloatTensor),edge_index.type(torch.LongTensor)).detach()\n",
        "      #print(\"qvalues\",q_values)\n",
        "      legal_q_values = q_values[legal_actions]\n",
        "      action = legal_actions[torch.argmax(legal_q_values)]\n",
        "      probs[action] = 1.0\n",
        "    return action, probs\n",
        "\n",
        "  def _get_epsilon(self, is_evaluation, power=1.0):\n",
        "    \"\"\"Returns the evaluation or decayed epsilon value.\"\"\"\n",
        "    if is_evaluation:\n",
        "      return 0.0\n",
        "    decay_steps = min(self._step_counter, self._epsilon_decay_duration)\n",
        "    decayed_epsilon = (\n",
        "        self._epsilon_end + (self._epsilon_start - self._epsilon_end) *\n",
        "        (1 - decay_steps / self._epsilon_decay_duration)**power)\n",
        "    return decayed_epsilon\n",
        "\n",
        "  def learn(self):\n",
        "    \"\"\"Compute the loss on sampled transitions and perform a Q-network update.\n",
        "    If there are not enough elements in the buffer, no loss is computed and\n",
        "    `None` is returned instead.\n",
        "    Returns:\n",
        "      The average loss obtained on this batch of transitions or `None`.\n",
        "    \"\"\"\n",
        "\n",
        "    if (len(self._replay_buffer) < self._batch_size or\n",
        "        len(self._replay_buffer) < self._min_buffer_size_to_learn):\n",
        "      return None     \n",
        "    transitions = self._replay_buffer.sample(self._batch_size)\n",
        "    actions =[]\n",
        "    rewards = []\n",
        "    are_final_steps = []\n",
        "    legal_actions_mask = []\n",
        "    q_values = []\n",
        "    target_q_values = []\n",
        "    for t in transitions:\n",
        "        info_states = t.info_state\n",
        "        size = int(math.sqrt(len(info_states)))\n",
        "        shape = (size,size)\n",
        "        edges = np.reshape(info_states, shape)\n",
        "        sA = sparse.coo_matrix(edges) \n",
        "        edge_index, _  = convert.from_scipy_sparse_matrix(sA)\n",
        "        data = torch.from_numpy(t.x.astype(np.float32))\n",
        "        q_values.append(torch.flatten(self._q_network(data.type(torch.FloatTensor),edge_index.type(torch.LongTensor))))\n",
        "        actions.append(t.action)\n",
        "        rewards.append(t.reward)\n",
        "        next_info_states = t.next_info_state \n",
        "        next_edges = np.reshape(next_info_states, shape)\n",
        "        next_sA = sparse.coo_matrix(next_edges) \n",
        "        next_edge_index, _  = convert.from_scipy_sparse_matrix(next_sA)\n",
        "        next_data = torch.from_numpy(t.next_x.astype(np.float32))\n",
        "        target_q_values.append(torch.flatten(self._target_q_network(next_data.type(torch.FloatTensor),next_edge_index.type(torch.LongTensor))))\n",
        "        are_final_steps.append(t.is_final_step)\n",
        "        legal_actions_mask.append(t.legal_actions_mask)\n",
        "    actions = torch.LongTensor(np.array(actions))\n",
        "    rewards = torch.Tensor(np.array(rewards))\n",
        "    are_final_steps = torch.Tensor(np.array(are_final_steps))\n",
        "    legal_actions_mask = torch.Tensor(np.array(legal_actions_mask))\n",
        "    self._q_values = q_values\n",
        "    self._target_q_values = target_q_values\n",
        "    illegal_actions = 1 - legal_actions_mask\n",
        "    illegal_logits = illegal_actions * ILLEGAL_ACTION_LOGITS_PENALTY\n",
        "    #print('Qvalues', self._q_values)\n",
        "    #print(\"illegallogits\",illegal_logits)\n",
        "    #print('targetvalues', self._target_q_values)\n",
        "    target=[]\n",
        "    prediction=[]\n",
        "    for i in range(self._batch_size):\n",
        "        max_next_q = torch.max(self._target_q_values[i]+ illegal_logits[i], dim=0)[0]\n",
        "        target.append((rewards[i] + (1 - are_final_steps[i]) * self._discount_factor * max_next_q).detach())\n",
        "        prediction.append(self._q_values[i][actions[i].item()])\n",
        "    target = torch.stack(target)\n",
        "    prediction = torch.stack(prediction)\n",
        "    #print(\"Target\",target)\n",
        "    #print(\"Prediction\",prediction)\n",
        "    loss = self.loss_class(prediction, target)\n",
        "    #print(\"Loss\",loss)\n",
        "    self._optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self._optimizer.step()\n",
        "    return loss\n",
        "    \n",
        "  @property\n",
        "  def q_values(self):\n",
        "    return self._q_values\n",
        "\n",
        "  @property\n",
        "  def replay_buffer(self):\n",
        "    return self._replay_buffer\n",
        "\n",
        "  @property\n",
        "  def loss(self):\n",
        "    return self._last_loss_value\n",
        "\n",
        "  @property\n",
        "  def prev_timestep(self):\n",
        "    return self._prev_timestep\n",
        "\n",
        "  @property\n",
        "  def prev_action(self):\n",
        "    return self._prev_action\n",
        "\n",
        "  @property\n",
        "  def step_counter(self):\n",
        "    return self._step_counter\n",
        "\n",
        "  def get_weights(self):\n",
        "    variables = [m.weight for m in self._q_network.model]\n",
        "    variables.append([m.weight for m in self._target_q_network.model])\n",
        "    return variables\n",
        "\n",
        "  def copy_with_noise(self, sigma=0.0, copy_weights=True):\n",
        "    \"\"\"Copies the object and perturbates it with noise.\n",
        "    Args:\n",
        "      sigma: gaussian dropout variance term : Multiplicative noise following\n",
        "        (1+sigma*epsilon), epsilon standard gaussian variable, multiplies each\n",
        "        model weight. sigma=0 means no perturbation.\n",
        "      copy_weights: Boolean determining whether to copy model weights (True) or\n",
        "        just model hyperparameters.\n",
        "    Returns:\n",
        "      Perturbated copy of the model.\n",
        "    \"\"\"\n",
        "    _ = self._kwargs.pop(\"self\", None)\n",
        "    copied_object = DQN(**self._kwargs)\n",
        "\n",
        "    q_network = getattr(copied_object, \"_q_network\")\n",
        "    target_q_network = getattr(copied_object, \"_target_q_network\")\n",
        "\n",
        "    if copy_weights:\n",
        "      with torch.no_grad():\n",
        "        for q_model in q_network.model:\n",
        "          q_model.weight *= (1 + sigma * torch.randn(q_model.weight.shape))\n",
        "        for tq_model in target_q_network.model:\n",
        "          tq_model.weight *= (1 + sigma * torch.randn(tq_model.weight.shape))\n",
        "    return copied_object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuXeWhnmuapz"
      },
      "source": [
        "# Training \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTPXQq4u064_",
        "outputId": "1c846875-85d4-4284-c66e-d618b4eadb74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "here\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 49/100000 [00:27<27:45:11,  1.00it/s]"
          ]
        }
      ],
      "source": [
        "# Copyright 2019 DeepMind Technologies Limited\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"DQN agents trained on Breakthrough by independent Q-learning.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from tqdm import tqdm\n",
        "from absl import app\n",
        "from absl import flags\n",
        "from absl import logging\n",
        "import numpy as np\n",
        "\n",
        "#from open_spiel.python import rl_environment\n",
        "from open_spiel.python.algorithms import random_agent\n",
        "\n",
        "\n",
        "# Training parameters\n",
        "save_every= int(5e3)                #\"Episode frequency at which the DQN agent models are saved.\")\n",
        "num_train_episodes = int(1e5)       #\"Number of training episodes.\")\n",
        "eval_every = int(5e3)                 #\"Episode frequency at which the DQN agents are evaluated.\")\n",
        "\n",
        "# DQN model hyper-parameters\n",
        "#hidden_layers =  [64,64]     #\"Number of hidden units in the Q-Network MLP.\"\n",
        "hidden_layers =  5     #\"Number of hidden units in the Q-Network MLP.\"\n",
        "replay_buffer_capacity= int(1e3)    #\"Size of the replay buffer.\"\n",
        "batch_size = 128                     #\"Number of transitions to sample at each learning step.\"\n",
        "learning_rate=0.01\n",
        "update_target_network_every=1000   #Value of tau how often we update the model\n",
        "learn_every=10\n",
        "discount_factor=0.9\n",
        "min_buffer_size_to_learn=1000\n",
        "epsilon_start=1.0\n",
        "epsilon_end=0.1\n",
        "epsilon_decay_duration=int(1e6)\n",
        "\n",
        "checkpoint_dir= './model/Different_Graph/model_differentGraph'\n",
        "\n",
        "\n",
        "def main(agents=None):\n",
        "    game = \"graph_attack_defend\"\n",
        "    env = Environment(game)\n",
        "    #info_state_size = env.observation_spec()[\"info_state\"][0]\n",
        "    feature_size = 5\n",
        "    num_actions = env.action_spec()[\"num_actions\"]\n",
        "    #hidden_layers_sizes = [int(l) for l in hidden_layers]\n",
        "    hidden_layers_sizes = hidden_layers\n",
        "    # pylint: disable=g-complex-comprehension\n",
        "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if agents == None:\n",
        "        print('here')\n",
        "        agents = [\n",
        "            DQN(\n",
        "                player_id=0,\n",
        "                state_representation_size=feature_size,\n",
        "                num_actions=num_actions,\n",
        "                hidden_layers_sizes=hidden_layers_sizes,\n",
        "                replay_buffer_capacity=replay_buffer_capacity,\n",
        "                learning_rate=learning_rate,\n",
        "                update_target_network_every=  update_target_network_every,\n",
        "                learn_every=learn_every,\n",
        "                discount_factor=discount_factor,\n",
        "                min_buffer_size_to_learn=min_buffer_size_to_learn,\n",
        "                epsilon_start=epsilon_start,\n",
        "                epsilon_end=epsilon_end,\n",
        "                epsilon_decay_duration=epsilon_decay_duration,\n",
        "                batch_size=batch_size)\n",
        "        ]\n",
        "    #agents.append(random_agent.RandomAgent(player_id=1, num_actions=num_actions))\n",
        "    reward_list = []\n",
        "    loss = []\n",
        "    for ep in tqdm(range(num_train_episodes)):\n",
        "        time_step = env.reset()\n",
        "        while not time_step.last():\n",
        "            agents_output = [agent.step(time_step) for agent in agents]\n",
        "            actions = [agent_output.action for agent_output in agents_output]\n",
        "            action_list = [actions[0], actions[0]]\n",
        "            time_step = env.step(action_list)\n",
        "        # Episode is over, step all agents with final info state.\n",
        "        reward_list.append(env.get_state._returns[0])\n",
        "        for agent in agents:\n",
        "            agent.step(time_step)\n",
        "        if (ep + 1) % eval_every == 0:\n",
        "            plt.plot(loss, label=\"loss\")\n",
        "            plt.ylabel(\"TD Loss\")\n",
        "            plt.xlabel(\"Iteration\")\n",
        "            plt.title(\"TD Loss vs Iteration\")\n",
        "            plt.savefig(\"./figure/loss/loss_DifferentGraph_\"+str(ep+1))\n",
        "            plt.clf()\n",
        "            plt.plot(reward_list, label=\"Max_Reward\")\n",
        "            plt.ylabel(\"Max Reward\")\n",
        "            plt.xlabel(\"Iteration\")\n",
        "            plt.title(\"Max reward vs Iteration\")\n",
        "            plt.savefig(\"./figure/reward/reward_DifferentGraph_\"+str(ep+1))\n",
        "            plt.clf()\n",
        "        if agents[0]._last_loss_value != None:\n",
        "                loss.append(agents[0]._last_loss_value.item())\n",
        "        if (ep + 1) % save_every == 0:\n",
        "            title = checkpoint_dir +\"_\"+str(ep+1)\n",
        "            torch.save(agents[0],title)\n",
        "    return agents, reward_list\n",
        "agents = None#[torch.load('./model/Different_Graph/model_differentGraph_30000')]\n",
        "agents,reward_list = main(agents)    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# END"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ATTACKER_DQN_GAT_Edge.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
